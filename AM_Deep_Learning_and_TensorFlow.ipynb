{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pylib.draw_nn import draw_neural_net_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = None\n",
    "\n",
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning and TensorFlow\n",
    "\n",
    "<!-- requirement: pylib/draw_nn.py -->\n",
    "<!-- requirement: pylib/__init__.py -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "In previous modules we have dealt with relatively simple algorithms and relatively small amounts of data. As projects get larger and more complex, it becomes more important to consider the performance of algorithms. [TensorFlow](https://www.tensorflow.org/) is a computational framework where your data and desired transformations are represented as a graph (a similar but more flexible incarnation of the scikit-learn pipeline). This has several advantages, including optimizations from lazy evaluation and enabling parallel computing.\n",
    "\n",
    "Let's go through an example workflow. For more details and reference, you can use:\n",
    "* https://www.tensorflow.org/how_tos/index.html\n",
    "* http://learningtensorflow.com/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lazy evaluation\n",
    "\n",
    "x = tf.constant([42, 212, 7, 13], name='x')\n",
    "y = tf.Variable(x ** 2, name='y')\n",
    "z = tf.Variable([0, 1, 2, 3], name='z')\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    for i in xrange(3):\n",
    "        session.run(model)\n",
    "        z += 1\n",
    "        print session.run(z)\n",
    "    print session.run(y)\n",
    "\n",
    "# Note that session.run(y) only computes the part of the graph necessary to calculate y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders are empty until you feed in data\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, 4])  # data type and dimension [rows, columns]\n",
    "y = -x  # operation\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_data = [[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8],]\n",
    "    result = session.run(y, feed_dict={x: x_data})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(0., name='x')\n",
    "threshold = tf.constant(5.)\n",
    "\n",
    "model = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(model)\n",
    "    while session.run(tf.less(x, threshold)):\n",
    "        x = x + 1\n",
    "        x_value = session.run(x)\n",
    "        print(x_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear model optimization\n",
    "x = tf.placeholder(\"float\")\n",
    "y = tf.placeholder(\"float\")\n",
    "w = tf.Variable([1.0, 2.0], name=\"w\")\n",
    "\n",
    "y_model = x * w[0] + w[1]\n",
    "\n",
    "error = tf.square(y - y_model)  # sum of squared error\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(error)\n",
    "\n",
    "model = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(model)\n",
    "    for i in range(1000):\n",
    "        x_value = np.random.rand()\n",
    "        y_value = x_value * 2 + 6\n",
    "        session.run(train_op, feed_dict={x: x_value, y: y_value})\n",
    "\n",
    "    w_value = session.run(w)\n",
    "    print(\"Predicted model: {a:.3f}x + {b:.3f}\".format(a=w_value[0], b=w_value[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "\n",
    "## What is deep learning?\n",
    "\n",
    "Deep learning is a branch of machine learning that tries to emulate the biological structure and function of the brain using artificial neural networks. These networks include: \n",
    "\n",
    "- Multilayer Perceptron Networks\n",
    "- Convolutional Neural Networks\n",
    "- Recurrent Neural Networks\n",
    "\n",
    "Additionally, these networks are hierarchical or multilayered, enabling them to model high-level abstractions in data. For this reason, deep learning is also called **hierarchical learning**. In this notebook, we will discuss the first type of network.\n",
    "\n",
    "There are benefits to using hierarchical models. In contrast to the performance of older machine learning algorithms, the performance of deep learning algorithms scales with the amount of data they are trained on -- the more data, the better the model. Consequently, deep learning algorithms typically outperform traditional ones. These models also have the ability to automatically extract features from data in a process called [feature learning](https://en.wikipedia.org/wiki/Feature_learning). This ability eliminates the need for a priori knowledge of the data to construct features, which is particularly useful when dealing with complex data such as images.  \n",
    "\n",
    "Deep learning has some pretty neat applications. Not only can we classify images with a high degree of accuracy, but we can also use deep learning algorithms to [generate captions](https://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.html), [summarize](https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html) and [translate](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html) text, [generate audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/), and [produce art](https://github.com/lengstrom/fast-style-transfer/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The perceptron\n",
    "The _perceptron_ is a linear decision boundary classifier\\* that trains by an iterative learning approach.\n",
    "\n",
    "The model works as follows:\n",
    "\n",
    "- **Input**: A data point. This point is transformed into an $n$-length \"feature vector\" $v$ $\\in R^n$, with each element describing the value of that particular feature.\n",
    "- **Output**: A classification, either -1 or 1.\n",
    "\n",
    "As mentioned above, the basic perceptron is linear, which means we can represent our model as another $n$-length \"weight vector\" $w$ - in the image below with input vector $v$, we\n",
    " - compute the inner product $<v,w> := u$\n",
    " - calculate $f(u)$, where $f$ is the _activation function_ (in a perceptron, the Heaviside step function, as in the diagram below).\n",
    " \n",
    " to obtain our prediction. It may also be instructive to think of this as matrix multiplication (with $v$ a `1xn` matrix and `w^T` an `nx1` matrix)- when we chain together perceptrons for a neural net, we can in fact represent each layer of the model as a matrix.\n",
    "\n",
    "![Perceptron](http://i.stack.imgur.com/KUvpQ.png)\n",
    "\n",
    "\\*In fact, one can use [kernel methods](https://en.wikipedia.org/wiki/Kernel_perceptron) (much like with SVMs) to attempt nonlinear classification with perceptrons.\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "The fact that the activation function is nonlinear is crucial.  This is what keeps the whole network from just being a linear transformation.  Any non-linearity will do though, so a number of different activation functions have been proposed.  Here are a few:\n",
    "\n",
    "The first perceptron, a single-layer neural network, desgined by Frank Rosenblatt in 1957, used the **Heaviside** or **step** function.  This is essentially equivalent to using a threshold with logistic regression.  While this if fine for predicting a class, it has slope 0 almost everywhere, and therefore is unsuitable for use with gradient descent algorithms.\n",
    "\n",
    "We have already seen the **sigmoid** function used in logistic regression.  In a sense, it smooths out the step function, allowing a usable gradient in the area near $x = 0$.  Because the function saturates at $\\pm\\infty$, the gradient goes to zero for large positive or negative inputs.  This can cause optimization algorithms to slow down.\n",
    "\n",
    "The average output of a sigmoid is 0.5, but it performs best when the average input is 0.  Thus, several layers of sigmoid neurons may push themselves away from optimal behavior.  One solution to this is use a **tanh** instead.  While the general shape is the same, its range is [-1, 1], so the output will on average be 0.\n",
    "\n",
    "The tanh will still have trouble with saturation of the signal.  Recently, many researchers have had success with the **rectified linear unit (ReLU)**: $f(x) = \\max(0, x)$.  While it might seem to combine the problems of the other functions (non-analytic points, zero derivatives, non-centered output), in practice it tends to be quite successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The XOR problem\n",
    "\n",
    "Research into artificial neurons dates to the late 40s, but it was not until 1969 that Martin Minsky and Seymour Papert pointed out that basic neurons were unable to reproduce the **exclusive-or** (XOR) function.  This boolean function of two boolean variables returns true if exactly one of its inputs is true:\n",
    "\n",
    "$$ \\mathrm{XOR}(0, 0) = \\mathrm{XOR}(1, 1) = 0 \\ \\ \\ \\ \\ \\ \\mathrm{XOR}(0, 1) = \\mathrm{XOR}(1, 0) = 1 $$\n",
    "\n",
    "Below, we create a related two-class classification problem, with one class clustered about (0, 0) and (1, 1), and the other about (0, 1) and (1, 0).  It would be quite easy to draw a boundary separating the two classes by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centers = np.array([[0, 0]] * 100 + [[1, 1]] * 100\n",
    "                   + [[0, 1]] * 100 + [[1, 0]] * 100)\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(0, 0.2, (400, 2)) + centers\n",
    "labels = np.array([[0]] * 200 + [[1]] * 200)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], c=labels, cmap=plt.cm.RdYlBu)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we build a simple logistic classifier for these data.  It takes two features of input and returns a prediction for the probability of being in class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_neural_net_fig([2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"features\")\n",
    "y_label = tf.placeholder(tf.float32, [None, 1], name=\"labels\")\n",
    "\n",
    "W = tf.Variable(tf.zeros([2, 1]), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([1]), name=\"biases\")\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, \n",
    "                                                              labels=y_label))\n",
    "train = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "predicted = tf.cast(tf.nn.sigmoid(y) > 0.5, np.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y_label), np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it for a little. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_vars()\n",
    "\n",
    "for i in range(300):\n",
    "    sess.run(train, feed_dict={x: data, y_label: labels})\n",
    "    if i % 30 == 0:\n",
    "        print sess.run([loss, accuracy],\n",
    "                       feed_dict={x: data, y_label: labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit odd.  The model seems to get stuck on 52% accuracy.  More notably, the entropy has barely improved at all.\n",
    "\n",
    "Let's visualize the predictions.  Note that the color scale only covers a portion of the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_labels = sess.run(tf.nn.sigmoid(y), feed_dict={x: data, y_label: labels})\n",
    "ww, bb = sess.run([W, b])\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], c=pred_labels, cmap=plt.cm.RdYlBu,\n",
    "            vmin=0.45, vmax=0.55)\n",
    "xx = np.linspace(-1, 2, 100)\n",
    "yy = -ww[0] / ww[1] * xx - bb / ww[1]\n",
    "plt.plot(xx, yy, 'k')\n",
    "plt.axis((-1,2,-1,2))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the problem becomes apparent: The model is returning nearly a 50% probability for each observation.  Therefore the entropy is stuck at just about $-\\log\\frac12 \\approx 0.6931$.\n",
    "\n",
    "We can draw a line at $p = 0.5$ to separate the two classes.  From the logistic function, we know this is equivalent to $x\\cdot W + b = 0$.  Thus, our model is attempting to draw a straight line through the plane to separate the two classes.  (In the general case, logistic regression separates the classes with a $(n-1)$-D hyperplane in $n$-D space.)  No line can do that in this case, so the model falls back to guessing 50% for each.\n",
    "\n",
    "We might wonder why the line wasn't chosen to separate one cluster from the other three.  This could have given us an accuracy approaching 0.75.  But remember that we are optimizing entropy, which is based on the probability estimates, not accuracy.  Because the probability grows the further we go from the threshold line, the penalty for the one cluster on the wrong side of the line would outweigh the gains from the two clusters put fully on the right side.\n",
    "\n",
    "## Multilayer Perceptron\n",
    "\n",
    "### Hidden Layers\n",
    "\n",
    "Let's try to combine these artificial neurons into a more complex configuration.  We'll make a network with a single **hidden layer** of size two.  That is, we will have two logistic regressions whose outputs are not visible.  Instead, they are fed into a third, visible neuron, whose output we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_neural_net_fig([2, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math behind this isn't as bad as it might seem at first.  All of the weights of the neurons in the hidden layer can be combined into a single $2\\times2$ matrix $W^{(1)}$.  The final neuron's weights will be in a $2\\times1$ matrix $W^{(2)}$.  The biases behave similarly.  Then our final probabilistic prediction is just\n",
    "\n",
    "$$ p_j = f_2\\bigg( f_1\\left( X_{ji} W^{(1)}_{ik} + b^{(1)}_k \\right) W^{(2)}_k + b^{(2)} \\bigg)$$\n",
    "\n",
    "We are using the Einstein notation: All repeated indices are implicitly summed over.  Both $f_1$ and $f_2$ represent the logistic function, which is taken to operate element-wise over tensors.\n",
    "\n",
    "The **backpropagation** algorithm, developed by Paul Werbos in 1975, points out that we can use gradient descent (or similar algorithms) to optimize all of the parameters in these sorts of expressions.  All it takes is successive applications of the chain rule.  In fact, there's nothing special we have to do to make use of it: TensorFlow's optimizers automatically work though the successive derivatives to generate the update rules.  All we have to do is set up the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 2\n",
    "W1 = tf.Variable(tf.random_normal([2, hidden_size], seed=42), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.zeros([hidden_size]), name=\"bias1\")\n",
    "\n",
    "hidden = tf.nn.sigmoid(tf.matmul(x, W1) + b1, name=\"hidden\")\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([hidden_size, 1], seed=24), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"bias2\")\n",
    "\n",
    "y = tf.matmul(hidden, W2) + b2\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y,\n",
    "                                                              labels=y_label))\n",
    "train = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "predicted = tf.cast(tf.nn.sigmoid(y) > 0.5, np.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y_label), np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's run it.  We need a few more steps to get all of the weights well-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_vars()\n",
    "\n",
    "for i in range(3000):\n",
    "    sess.run(train, feed_dict={x: data, y_label: labels})\n",
    "    if i % 300 == 0:\n",
    "        print sess.run([loss, accuracy],\n",
    "                       feed_dict={x: data, y_label: labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the improved accuracy by examining our predictions.  This time, we plot the probability as a background field, so we can compare the actual labels in marker colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mesh = np.column_stack(a.reshape(-1) for a in np.meshgrid(np.r_[-1:2:100j], np.r_[-1:2:100j]))\n",
    "ymesh = sess.run(tf.nn.sigmoid(y), feed_dict={x: mesh})\n",
    "\n",
    "plt.imshow(ymesh.reshape(100,100), cmap=plt.cm.RdYlBu, origin='lower',\n",
    "           extent=(-1, 2, -1, 2), vmin=0, vmax=1)\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=plt.cm.RdYlBu,\n",
    "            edgecolor='w', lw=1)\n",
    "plt.axis((-1, 2, -1, 2))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is that stripe created?  We can get some understanding by looking at the weights of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ww1 = sess.run(W1)\n",
    "hmesh = sess.run(hidden, feed_dict={x: mesh})\n",
    "\n",
    "for i in xrange(hidden_size):\n",
    "    plt.subplot(1, hidden_size, i+1)\n",
    "    plt.imshow(hmesh[:, i].reshape((100, 100)), origin='lower', cmap=plt.cm.RdYlBu,\n",
    "               extent=(-0.5,1.5,-0.5,1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how each hidden neuron is just defining a line through the feature space.  Each line defines one side of the strip.\n",
    "\n",
    "Note that the colors are inverted from the final probabilities.  We can understand what's going on by examining the weights in the second layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Classifying handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch processing\n",
    "\n",
    "**Minibatch processing** trains on a \"minibatch\", a subset of the whole data, at a time.  This increases the accuracy of the gradient calculation, reducing the noise in each step, while still avoiding much of the duplication in the whole set.  Minibatches can often be about as fast as individual steps&mdash;the parallelism available in modern CPUs and especially GPUs is wasted if we are calculating a single row at a time.\n",
    "\n",
    "A batch size of 100 is usually enough to smooth out the noise.  In our case, the full data is not so much larger than the batch size, so there is little difference in performance. We call 1 full iteration over the training set an **epoch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_PIXELS= 28 * 28\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.5\n",
    "\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Weights and Biases\n",
    "\n",
    "As a reminder, we want to initialize our weights with random values to break symmetry between neurons in a hidden layer. Additionally, we want to choose small values to avoid the **gradient vanishing problem**, where the weighted sum of the inputs (plus a bias) fall on the flat portion of the sigmoid curve. What is the proper scale of the weights?  Most of our activation functions have their best response for inputs of $\\mathcal O(1)$.  If we have $m$ random inputs, each of $\\mathcal O(1)$, we expect their sum to scale as $\\sqrt m$.  Therefore, weights are often chosen randomly with a mean of zero and standard deviation of $1/\\sqrt m$.\n",
    "\n",
    "For very large layers, this gives rather small weights.  An alternative approach is to only provide $k < m$ non-zero weights when initializing neurons.  This scheme, known as **sparse initialization**, provides more diversity amongst the neurons at initialization.  It can, however, also produce very slow convergence as \"incorrect\" choices of non-zero weights have the be removed.\n",
    "\n",
    "In the code below, we initialize our weights by sampling from a truncated normal distribution, where any weights greater than 2 standard deviations from the mean is re-picked. We also initialize the biases to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initializer(shape):\n",
    "    return tf.truncated_normal(shape, stddev=shape[0]**-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, N_PIXELS], name=\"pixels\")\n",
    "y_label = tf.placeholder(tf.float32, [None, 10], name=\"labels\")\n",
    "\n",
    "W1 = tf.Variable(initializer([N_PIXELS, hidden_size]), name=\"weights\")\n",
    "b1 = tf.Variable(tf.zeros([hidden_size]), name=\"biases\")\n",
    "\n",
    "hidden = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(initializer([hidden_size, 10]), name=\"weights2\")\n",
    "b2 = tf.Variable(tf.zeros([10]), name=\"biases2\")\n",
    "\n",
    "y = tf.matmul(hidden, W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y,\n",
    "                                                              labels=y_label))\n",
    "train = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(y_label, 1)), tf.float32))\n",
    "\n",
    "reset_vars()\n",
    "for i in range(10000):\n",
    "    batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "    sess.run(train,\n",
    "             feed_dict={x: batch_x, y_label: batch_y})\n",
    "    if i % 1000 == 0:\n",
    "        print \"Test: \", sess.run([loss, accuracy],\n",
    "                                 feed_dict={x: mnist.test.images,\n",
    "                                            y_label: mnist.test.labels})\n",
    "        print \"Train:\", sess.run([loss, accuracy],\n",
    "                                 feed_dict={x: mnist.train.images, \n",
    "                                            y_label: mnist.train.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(y, 1)\n",
    "\n",
    "def predict(idx):\n",
    "    image = mnist.test.images[idx]\n",
    "    return sess.run(prediction, feed_dict={x: [image]})\n",
    "\n",
    "idx = 0\n",
    "actual = np.argmax(mnist.test.labels[idx])\n",
    "print (\"Predicted: %d, Actual: %d\" % (predict(idx), actual))\n",
    "plt.imshow(mnist.test.images[idx].reshape((28,28)), cmap=plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
