{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import quandl\n",
    "import dill\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Time Series\n",
    "<!-- requirement: images/ts_xval.png -->\n",
    "<!-- requirement: projects/timeseries-project -->\n",
    "<!-- requirement: small_data/quandl_oil.pkl -->\n",
    "\n",
    "Time series prediction or forecasting presents its own challenges which are different from machine-learning problems.  Like many other classes of problems, it also presents a number of special features which are common.\n",
    "\n",
    "In general, while we could use regression to make predictions, the goal of time series analysis is to take advantage of the temporal nature of the data to make more sophisticated models. To properly forecast events, we need to implement techniques to find and model the long term trends (**drift**), periodic signals (**seasonality**), and residual noise in our data. We will also need to preserve the order of our observations while cross-validating our model.\n",
    "\n",
    "We will follow this general procedure for modeling time series:\n",
    "1. Plot the time series. Look for trends, seasonality, and step changes.\n",
    "2. Model and remove drift and seasonal components. \n",
    "3. Fit a model to the residuals. \n",
    "4. Add drift and seasonality back into the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Trends in time series data\n",
    "\n",
    "As we explain these, let's first define our time-series as\n",
    "\n",
    "$$ \\{X_t, t = \\ldots, -1, 0, 1, \\ldots\\} \\, . $$\n",
    "\n",
    "We take $\\varepsilon \\sim N(0, \\sigma^2)$ to be i.i.d. normal errors.  \n",
    "1. **Stationarity**.  Informally, this means that the distribution of the $X_t$ is independent of time $t$.  Formally, a time-series is stationary if for all $k \\ge 0$ and $t$, the following two $k$-tuples have the same distribution:\n",
    "$$ (X_0,\\ldots,X_k) \\sim (X_t,\\ldots,X_{t+k}) $$\n",
    "1. **Drift**.  One reason a time-series might not be stationary is that it possess a drift.  For example, we know that prices tend to creep up with inflation.  Mathematically, we might represent the (log) prices as\n",
    "$$ X_t = \\mu t + \\varepsilon_t $$\n",
    "1. **Seasonality**.  Another reason a time-series might not be stationary is that it posseses a seasonal component.  For example, we know that the temperature increases in the summer and decreases in the winter.  A simple model of this might be\n",
    "$$ X_t = \\alpha \\sin(\\omega t) + \\beta \\cos(\\omega t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross-validation for time series\n",
    "Cross validation is very different for time series than with other machine-learning problem classes.  In normal machine-learning, we select a random subset of data as a validation set to estimate accuracy of your measure.  In time series, we have to consider the problem that we are trying to solve is often to predict a value in the future.  Therefore, the validation data always has to occur *after* the training data.  As a simple example, consider that it would not be very useful to have a predictor of tomorrow's temperature that depended on the temperature the day after. In general:\n",
    "\n",
    "- We cannot just pick data points at random, because there might be lagged indicators / seasonal effects / etc. that force us to work with contiguous blocks of time.  \n",
    "- We cannot blindly chop by e.g., month or year without some thought: There could be seasonal effects so that Decembers are always different.  There could be systemic \"regime changes\" that mean that cutting at a given date is inappropriate, or known and time-limited effects that last a year (or fraction thereof).   For instance, the years 1991 and 2008 in this data set.\n",
    "- We cannot have our testing set occur before our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![ts cross-validation illustration](images/ts_xval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We usually handle this by doing a **sliding-window validation method**.  That is, we train on the last $n$ data points and validate the prediction on the next $m$ data points, sliding the $n + m$ training / validation window in time.  In this way, we can estimate the parameters of our model.  To test the validity of the model, we might use a block of data at the end of our time series which is reserved for testing the model with the learned parameters.\n",
    "\n",
    "Another common technique is to use [**forward chaining**](http://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection).\n",
    "\n",
    "**Question:** What are the strengths and weaknesses of using a fixed vs. rolling window?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Modeling drift\n",
    "\n",
    "For examples, we will use two different datasets, weather and oil prices. The weather data is available locally in the `projects/` subdirectory, and the oil data will be retrieved on-the-fly from Quandl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# To really use the Quandl API, you should get an authtoken.  Limited usage doesn't require it.\n",
    "\n",
    "authtoken = None\n",
    "# authtoken = \"your token here\"\n",
    "\n",
    "def get_quandl(what):\n",
    "    \"\"\" \n",
    "    Wrapper around Quandl requests, using authtoken only if available\n",
    "    \"\"\"\n",
    "    if authtoken:\n",
    "        return quandl.get(what, authtoken=authtoken)\n",
    "    else:\n",
    "        return quandl.get(what)\n",
    "    \n",
    "#oil = get_quandl(\"DOE/RWTC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open('small_data/quandl_oil.pkl', 'wb') as f:\n",
    "     #dill.dump(oil, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"small_data/quandl_oil.pkl\", \"r\") as fin:\n",
    "    oil = dill.load(fin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will start of by visually inspecting our time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "oil.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the plot of oil value over time, we see that the trend is exponential. We can model exponential trends using a linear regression model if we transform the data first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.log(oil).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add a constant field for the linear regression\n",
    "def add_constant(X):\n",
    "    X['const'] = pd.Series(np.ones(len(X.index)), index=X.index)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "oil['Julian'] = oil.index.to_julian_date()\n",
    "oil = add_constant(oil) \n",
    "\n",
    "# We can actually train a simple exponential model using the log(value), then train further models on the error.\n",
    "exponential_model = sklearn.linear_model.Ridge().fit( \n",
    "    X=oil[['Julian', 'const']], \n",
    "    y=np.log(oil['Value'])\n",
    ")\n",
    "\n",
    "exp_model_df = oil\n",
    "exp_model_df['Exponential_Model'] = np.exp(exponential_model.predict(oil[['Julian', 'const']]))\n",
    "exp_model_df['Log_Error_Exponential'] = np.log(oil['Value'] / oil['Exponential_Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "exp_model_df[['Value', 'Exponential_Model']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Modeling seasonality\n",
    "\n",
    "Let's take a look at the temperature dataset and see if we can spot any periodic signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temps = pd.read_csv(\"projects/timeseries-project/data/raw/temperatures.csv\", \n",
    "                    index_col=0,\n",
    "                    names=[\"Temperature\"],\n",
    "                    parse_dates=True,\n",
    "                    date_parser=lambda u: pd.datetime.strptime(u, \"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temps = temps.asfreq('60Min', method='ffill')\n",
    "print temps[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temps.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can clearly see that there is seasonality in our temperature data. In order to model or remove these signals from our time series, we need to determine their amplitudes and frequencies. Since we are all familiar with daily and yearly temperature swings, we can probably guess what those frequencies are. However, there is a formal approach we can take to find these frequencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fourier Analysis\n",
    "\n",
    "Any periodic signal can be represented as the sum of a number of sine waves with varying amplitude, phase, and frequency.  A time series can be convered into its frequency components with the mathematical tool known as the *Fourier transform*.  As we are dealing with sampled data, we must use the discrete version.  \n",
    "\n",
    "For $N$ uniformly sampled datapoints $x_j$ ($j = 0$,...,$N-1$)\n",
    "\n",
    "$$ x_j = \\frac{1}{N} \\sum_{k=0}^{N-1} X_k e^{i2 \\pi jk/N} $$\n",
    "$$ X_k = \\sum_{j=0}^{N-1} x_j e^{-i2 \\pi jk/N} $$\n",
    "\n",
    "The common algorithm for computing discrete transforms in the *fast Fourier transform*, usually abbreviated *FFT*.\n",
    "\n",
    "The output of a FFT can be thought of as a representation of all the frequency components of your data. In some sense it is a histogram with each “frequency bin” corresponding to a particular frequency in your signal. Each frequency component has both an amplitude and phase, and is represented as a complex number. In the equations above, $X_k$ can be thought of as the \"ammount\" of frequency $k$ in the signal $x$. Generally, we care only about the amplitude, given by the modulus of that complex term.  As we are dealing with discrete data, the signal is bounded, and interpretation of the axes requires some consideration.\n",
    "\n",
    "For our purposes, it's enough to know that any strong signals such as peaks in the frequency domain represent some periodic (sinusoidal) behavior in the time domain. You can then go through the process of determining the wavelength (periodicity) based on how the FFT was taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "\n",
    "fft = fftpack.fft(temps.Temperature - temps.Temperature.mean())\n",
    "plt.plot(np.abs(fft))\n",
    "plt.title(\"FFT of temperature data\")\n",
    "plt.xlabel('# Cycles in full window of data (~13 years)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that we subtract the mean before computing the FFT.  Otherwise, there would be a large zero-frequency component.\n",
    "\n",
    "The x-axis is showing frequency.  Low frequencies, corresponding to long times, are on the left; high freqencies and short times are on the right.  The lowest non-zero frequency measured has a single cycle over the full 13-year period, corresponding to a frequency of $1/13\\, y^{-1}$.  The highest frequency is that of the sampling, $1\\, h^{-1}$.\n",
    "\n",
    "The discrete Fourier transform of real data is symmetric about the center of its frequency range.  Due to the problem of *aliasing*, it cannot distinguish signals with a frequency above half its sampling frequency from those below.  This limiting frequency is called the **Nyquist frequency**, and the results of a FFT for frequencies above it should be ignored.\n",
    "\n",
    "More formally, the Nyquist frequency is the highest frequency that can be resolved given your sampling rate.\n",
    "\n",
    "$$ \\nu_N = \\frac{1}{2 \\Delta t} $$\n",
    "\n",
    "The following plot demonstrates that, with a sampling frequency of 1, it is impossible to distinguish between signals above and below the Nyquist frequency of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_alias(f=0.2):\n",
    "    t = np.linspace(0, 10, 1000)\n",
    "    ts = np.arange(0, 11)\n",
    "    plt.plot(t, np.sin(2*np.pi * f * t), label='f = %0.2f' % f)\n",
    "    plt.plot(t, -np.sin(2*np.pi * (1 - f) * t), label='f = %0.2f' % (1 - f))\n",
    "    ml, sl, bl = plt.stem(ts, np.sin(2*np.pi * f * ts), label='Sampled signal')\n",
    "    plt.setp(ml, 'markerfacecolor', 'r')\n",
    "    plt.setp(sl, 'color', 'r')\n",
    "    plt.setp(bl, visible=False)\n",
    "    plt.xticks(ts)\n",
    "    plt.ylim(-2,2)\n",
    "    plt.legend()\n",
    "\n",
    "interact(plot_alias, f=(0,0.5,0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(1./13 * np.arange(len(fft)), np.abs(fft))\n",
    "plt.title(\"FFT of temperature data (Low Frequencies zoom)\")\n",
    "plt.xlim([0,5])\n",
    "plt.xlabel('Frequency ($y^{-1}$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** Is the above showing the yearly or daily seasonality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "daily_cycles_in_13_years = 13*365\n",
    "\n",
    "plt.plot(1./13 * np.arange(len(fft)), np.abs(fft))\n",
    "plt.title(\"FFT of temperature data (High Frequencies zoom)\")\n",
    "plt.ylim([0,400000])\n",
    "plt.xlim([365 - 30, 365 + 30])\n",
    "plt.xlabel('Frequency ($y^{-1}$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** What do you expect the FFT of the oil data to look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fft = fftpack.fft(oil['Log_Error_Exponential'][(oil.index.year > 1992) & (oil.index.year < 2010)])\n",
    "plt.plot(np.abs(fft))\n",
    "plt.ylim([0, 700])\n",
    "plt.xlim([0, 400])\n",
    "plt.title('FFT of Log Error in Oil Prices')\n",
    "plt.xlabel('# Cycles in window of data (~17 years)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are two ways to handle seasonality.  Seasonality features are nice because they are good at projecting arbitrarily far into the future.\n",
    "\n",
    "1. The simplest (and perhaps most robust) is to have a set of indicator variables for each month.\n",
    "\n",
    "  **Question**: Should month be a continuous or categorical variable?\n",
    "\n",
    "1. Since we know that temperature is roughly sinusoidal, we know that a reasonable model might be\n",
    "$$ y_t = k \\sin\\left( \\frac{t - t_0}{T} \\right) + \\epsilon$$\n",
    "where $k$ and $t_0$ are parameters to be learned and $T$ is one year for seasonal variation.  While this is linear in $k$, it is not linear in $t_0$.  However, we know from Fourier analysis, that the above is equivalent to \n",
    "$$ y_t = A \\sin\\left( \\frac{t}{T} \\right) + B \\cos\\left( \\frac{t}{T} \\right) + \\epsilon$$\n",
    "which is linear in $A$ and $B$.  This can be solved using a linear regression.\n",
    "\n",
    "**Question:** Does it always make sense to combine seasonal frequencies additively? When would it not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example: Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temps['Julian'] = temps.index.to_julian_date()\n",
    "temps['const'] = 1\n",
    "temps['sin(year)'] = np.sin(temps['Julian'] / 365.25 * 2 * np.pi)\n",
    "temps['cos(year)'] = np.cos(temps['Julian'] / 365.25 * 2 * np.pi)\n",
    "temps['sin(6mo)'] = np.sin(temps['Julian'] / (365.25 / 2) * 2 * np.pi)\n",
    "temps['cos(6mo)'] = np.cos(temps['Julian'] / (365.25 / 2) * 2 * np.pi)\n",
    "temps['sin(day)'] = np.sin(temps.index.hour / 24.0 * 2* np.pi)\n",
    "temps['cos(day)'] = np.cos(temps.index.hour / 24.0 * 2* np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temps['Temperature'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temps['Temperature'].shift(-1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# notice that the goal is our variable shifted by the desired period of time\n",
    "temps['Goal'] = temps['Temperature'].shift(-24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cut_year = 2012\n",
    "\n",
    "train = temps[temps.index.year < cut_year].dropna(how='any')\n",
    "test  = temps[temps.index.year >= cut_year].dropna(how='any')\n",
    "\n",
    "regress = sklearn.linear_model.LinearRegression().fit( \n",
    "        X=train[['Temperature', 'sin(year)', 'cos(year)', 'sin(6mo)', 'cos(6mo)', 'sin(day)', 'cos(day)']], \n",
    "        y=train['Goal'])\n",
    "\n",
    "test['Predicted_Value'] = regress.predict(X=test[['Temperature', 'sin(year)', 'cos(year)', 'sin(6mo)', 'cos(6mo)', 'sin(day)', 'cos(day)']] )\n",
    "\n",
    "(test['Goal'] - test['Predicted_Value']).plot()\n",
    "print sklearn.metrics.mean_squared_error(test['Goal'], test['Predicted_Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To remove these frequencies, you can apply a filter to your time series. Filters are discussed in further detail in the digital signals notebook. You can also check out [this](http://stackoverflow.com/questions/39799821/how-to-remove-frequency-from-signal) stack overflow post. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Modeling \"noise\"\n",
    "\n",
    "We can improve our temperature predictions by modeling the noise. In this context, the noise is caused by weather patterns that are superimposed on the background climatology. If you are trying to predict the temperature a few hours into the future (what is known as _nowcasting_), you can imagine that the weather from two months ago is not as good of an indicator as the weather from three hours ago. How far back in your time series should you look? You can autocorrelate your time series (with drift and seasonality removed) to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Autocorrelation\n",
    "\n",
    "Let's review some definitions:\n",
    "\n",
    "* **Covariance** ($s_{xy}$)\n",
    "\n",
    "$$ \\mbox{Cov}(X,Y) = E[(X - E(X))(Y - E(Y))] = \\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y}) $$\n",
    "\n",
    "* **Variance** ($s_x^2$) is equal to Cov(X,X).\n",
    "\n",
    "$$ \\mbox{Var}(X) = E[(X - E(X))^2] = \\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\bar{x})^2 $$\n",
    "\n",
    "* **Standard Deviation** ($s_x$) is the square root of the variance. \n",
    "* **Correlation** ($\\rho$)\n",
    "\n",
    "$$ \\mbox{Cor}(X,Y) = \\frac{s_{xy}}{s_x s_y} $$\n",
    "\n",
    "If we replace time series Y with a laged version of X, we can compute the autocovariance and autocorrelation of X at that given lag, $k$. \n",
    "\n",
    "* **Autocovariance**\n",
    "\n",
    "$$ \\gamma_k = \\mbox{Cov}(X_t, X_{t+k}) $$\n",
    "\n",
    "* **Autocorrelation**\n",
    "\n",
    "$$ \\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\frac{\\gamma_k}{s_x^2}$$\n",
    "\n",
    "**Question**: The definitions above apply to data sampled from some parent population. How do the equations change if you consider the entire population?\n",
    "\n",
    "It is useful to plot the autocorrelation for different values of $k$. Pandas includes a function which will create an autocorrelation plot for any data series.  It includes lines showing the 95% (solid) and 99% (dashed) confidence levels of the zero-correlation hypothesis. Let's look at this plot for the oil data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "\n",
    "autocorrelation_plot(oil['Log_Error_Exponential'])\n",
    "plt.xlabel('Lag (days)')\n",
    "plt.axvline(365, color = 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice that the autocorrelation is one at zero lag. This will be true for every time series. Why? Also notice that the autocorrelation drops to zero at around 1500 days. We have discovered that if we want to predict future oil values, we should only consider \"noise\" up until 1500 days prior. The time associated with this zero crossing is called the characteristic time scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Moving Average\n",
    "\n",
    "Once we know how closely points in the time series are correlated with recent points, we can predict arbitrarily far into the future by substituting the moving average predictions in for values as necessary. For example, if we only knew the values up through time x, we would need to estimate Value[x + 1] before calculating Value[x + 2] = f(Value[x + 1], Value[x], Value[x - 1], ...).\n",
    "\n",
    "There are many types of [moving average](http://en.wikipedia.org/wiki/Moving_average). Simple variations on this include [exponential moving averages](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average) and [rolling window averages](https://en.wikipedia.org/wiki/Moving_average#Cumulative_moving_average). While these features might give more accurate data, they are only useful for short-term projections (as compared to seasonality and longer-term trends). As you'll see, they're also quite good for smoothing purposes. We will be computing the exponentially weighted moving average here:\n",
    "\n",
    "$$E_t = \\alpha X_t + (1-\\alpha)E_{t-1}$$\n",
    "\n",
    "Note that as the smoothing factor $\\alpha$ increases, older observations are discounted faster. In Pandas, the ewm() method allows for specifying the decay as a function of $\\alpha$ in three ways:\n",
    "1. The center of mass (*com*), giving $\\alpha = \\frac{1}{1 + com}$\n",
    "2. The *span*, giving $\\alpha = \\frac{2}{1 + span}$\n",
    "3. The *halflife*, giving $\\alpha = 1 - \\exp{\\frac{\\log(0.5)}{halflife}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example: Predicting Oil Prices\n",
    "\n",
    "Let's pick up the oil price data using the residual after the exponential trend is removed. Since we know that something crazy happened in 2008 (financial meltdown) and something else crazy happened in 1992 (Iraq War), we'll restrict our analysis to between those two dates. We will attempt to build a model to predict oil prices 3 months in the future.  A column of oil prices shifted by in time by 3 months will serve as our labels for testing and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Create a column with \"true\" (future) values\n",
    "PERIOD_MONTH = 20\n",
    "PREDICTION_LAG = 3 * PERIOD_MONTH\n",
    "\n",
    "CUT_YEAR = 2008\n",
    "\n",
    "# notice that the goal is our variable shifted by the desired period of time\n",
    "oil['Actual'] = oil['Value'].shift(-PREDICTION_LAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "oil['Value'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "oil['Value'].shift(-1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "oil.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We build a model to predict the residuals of the exponential model three months out, from the last and current residuals, as well as 3-month rolling averages for the price and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Cross validation -- simple model\n",
    "\n",
    "#Train/Test\n",
    "train = oil[oil.index.year < CUT_YEAR]\n",
    "test = oil[oil.index.year >= CUT_YEAR]\n",
    "\n",
    "# Reporting function\n",
    "def summarize_errors(test_me):\n",
    "    error_pct = (test_me['Actual'] - test_me['Model']) / test_me['Actual']\n",
    "\n",
    "    print error_pct.describe()\n",
    "    error_pct.plot()\n",
    "    plt.title('% error')\n",
    "    plt.show()\n",
    "\n",
    "    error_pct.hist(bins=100, normed=True, label = 'Counts')\n",
    "    x = np.arange(-1, 1, 0.001)\n",
    "    plt.plot(x, sp.stats.norm(loc=error_pct.mean(),\n",
    "                              scale=error_pct.std()).pdf(x),\n",
    "                              linewidth=3, color='red', label='Normal Dist')\n",
    "    plt.title('Histogram of % errors')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print sklearn.metrics.mean_squared_error( test_me['Actual'], test_me['Model'] )\n",
    "\n",
    "# Train the regression\n",
    "def frame_to_feats(frame):\n",
    "    feats = pd.DataFrame()\n",
    "    \n",
    "    feats['LEE'] = frame['Log_Error_Exponential']\n",
    "    feats['LEE_1'] = frame['Log_Error_Exponential'].shift(1)\n",
    "    feats['dLEE_avg'] = pd.Series.rolling(frame['Value'].diff(), window=3*PERIOD_MONTH).mean()\n",
    "    feats['vol_avg'] = pd.Series.ewm(frame['Value'], span=3*PERIOD_MONTH).var(bias=False)\n",
    "    \n",
    "    feats['Actual_LEE'] = frame['Log_Error_Exponential'].shift(-PREDICTION_LAG)\n",
    "    return add_constant(feats)\n",
    "\n",
    "feats = frame_to_feats(train).dropna(how='any')\n",
    "X_train = feats.drop('Actual_LEE', axis=1).values\n",
    "y_train = feats['Actual_LEE'].values\n",
    "regress = sklearn.linear_model.LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "feats = frame_to_feats(test).dropna(how='any')\n",
    "X_test = feats.drop('Actual_LEE', axis=1).values\n",
    "y_test = feats['Actual_LEE'].values\n",
    "feats['Predicted_LEE'] = regress.predict(X_test)\n",
    "\n",
    "test = feats.join(test, rsuffix='_r').dropna(how='any')\n",
    "test['Simple_Model'] = np.exp(test['Predicted_LEE']) * test['Exponential_Model']\n",
    "\n",
    "# Report\n",
    "test_me = test[['Actual', 'Simple_Model']].dropna(how='any') \\\n",
    "                                          .rename(columns={'Simple_Model': 'Model'})\n",
    "summarize_errors(test_me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**: Build a temperature predictor using just these other features.  Try experimenting with non-linear models like random forests as well as linear ones.  While it may not be as effective, it can add signal when combined with the above models.  *Notes*:\n",
    "1. Make sure you don't accidently give yourself access to concurrent data: you cannot use the wind direction this hour to predict the temperature, you have to use past wind direction information.\n",
    "1. Try combining the signal using non-linear as well as linear techniques.  Is there a reason you might think that linear techniques work pretty well for aggregation of different signals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Using external data sources as features\n",
    "\n",
    "So far, our features have all depended on the date and previous temperature.  However, we can add in other variables into our prediction. Ideally we'd like to find signal in additional data sources that accounts for some of the error; to try to conceptually explain sources of error or skews in the distribution of error; etc.  Here are examples of other data sources we might try:\n",
    "\n",
    "  - Other financial indicators (e.g., interest rates, volatilities, related commodities)\n",
    "  - Non-financial indicators (e.g., weather, indicators for weather patterns / wars, geopolitical data like gdelt).\n",
    "  \n",
    "We'll show the example of trying to use equities volatility data (in the form of the VIX index) -- note that this will not help. Can you do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ng_fut = get_quandl(\"CHRIS/CME_NG1\")\n",
    "vix = get_quandl(\"YAHOO/INDEX_VIX\")\n",
    "\n",
    "oil['vix'] = vix['Adjusted Close']\n",
    "oil['ng_fut'] = ng_fut['Settle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at linear correlations between new data and the stationary residual of the oil data\n",
    "print oil['Log_Error_Exponential'].corr(oil['vix'])  # Our error term does correlate negatively with vix...\n",
    "print oil['Log_Error_Exponential'].corr(oil['ng_fut'])\n",
    "\n",
    "oil['vix'].plot()\n",
    "plt.show()\n",
    "\n",
    "oil['ng_fut'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Cross validation -- complex model -- notice that we have overfit!\n",
    "\n",
    "#Train/Test\n",
    "train = oil[oil.index.year < CUT_YEAR]\n",
    "test = oil[oil.index.year >= CUT_YEAR]\n",
    "\n",
    "\n",
    "# Train the regression\n",
    "def frame_to_feats(frame):\n",
    "    feats = pd.DataFrame()\n",
    "    \n",
    "    feats['LEE'] = frame['Log_Error_Exponential']\n",
    "    feats['dLEE_avg'] = pd.Series.rolling(frame['Value'].diff(), window=3*PERIOD_MONTH).mean()\n",
    "    feats['vol_avg'] = pd.Series.ewm(frame['Value'], span=3*PERIOD_MONTH).var(bias=False)\n",
    "    \n",
    "    feats['ng_fut'] = frame['ng_fut']\n",
    "    feats['vix'] = frame['vix']\n",
    "    \n",
    "    feats['Actual_LEE'] = frame['Log_Error_Exponential'].shift(-PREDICTION_LAG)\n",
    "    return add_constant(feats)\n",
    "    \n",
    "\n",
    "feats = frame_to_feats(train).dropna(how='any')\n",
    "regress = sklearn.linear_model.LinearRegression().fit( \n",
    "        X=feats.drop('Actual_LEE', axis=1), \n",
    "        y=feats['Actual_LEE'])\n",
    "\n",
    "# Predict\n",
    "\n",
    "feats = frame_to_feats(test).dropna(how='any')\n",
    "feats['Predicted_LEE'] = regress.predict(feats.drop('Actual_LEE', axis=1))\n",
    "\n",
    "test = feats.join(test, rsuffix='_r').dropna(how='any')\n",
    "test['Complex_Model'] = np.exp (test['Predicted_LEE']) * test['Exponential_Model']\n",
    "\n",
    "# Report\n",
    "test_me = test[['Actual', 'Complex_Model']].dropna(how='any') \\\n",
    "                                           .rename(columns = {'Complex_Model': 'Model'})\n",
    "summarize_errors(test_me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Open-ended brainstorming / exercises\n",
    "\n",
    "1. What happens to the results above if we change our \"cut point\" to say 2010?  What's the moral of this story...\n",
    "\n",
    "2. Play around with the previous \"Complex\" model, and see if you can improve it.  What happens, for instance, if you get rid of the 'vix' signal.  Why do you think this might be the case?\n",
    "\n",
    "3. What are some other \"simplest\" models we could have tried? e .g., linear regression just on 'Value' rather than going through this log stuff.  Try some of them -- how do they perform?\n",
    "\n",
    "4. Carry out a similar analysis for the temperature data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## More advanced time series modeling frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If a time series can be made stationary (trendless) by differencing or by the methods above, it is common to model it using some combination of auto-regressive terms (weighted average over some recent values) and moving average terms (weighted average over some recent errors) of different orders. The number of terms can be determined through various methods and rules of thumb. [Read more.](http://people.duke.edu/~rnau/411arim.htm)\n",
    "\n",
    "There is an entire literature on [auto-regressive models](https://en.wikipedia.org/wiki/Autoregressive_model) as well as [auto-regressive moving average models](https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model).  Read up about them on wikipedia to get a better idea.  In python, they are implemented in [Statsmodels](https://github.com/statsmodels/statsmodels/).\n",
    "\n",
    "* **Moving Average (MA) model**. This model works under the assumption that the output variable depends linearly on the current and past values of noise. \n",
    "$$ X_t = \\mu + \\varepsilon_t + \\sum_{s=1}^q \\theta_s \\varepsilon_{t-s} $$\n",
    "\n",
    "Here, $\\mu$ is the mean of the time series, the $\\varepsilon$'s are the noise, and the $\\theta$'s are the model parameters. We choose $q$ based on the where the autocorrelation function of MA($q$) becomes zero at lag $q+1$. We can fit the model parameters $\\theta$ using least squares regression or maximum likelihood estimation. \n",
    "\n",
    "* **Auto-regressive Model (AR)**.  Informally, auto-regressive processes \"regress to the mean\".  This model works under the assumption that the output depends linearly on the previous values and a stochastic term. \n",
    "\n",
    "$$ X_t = c + \\sum_{r=1}^p \\varphi_r X_{t-r} + \\varepsilon_t $$\n",
    "\n",
    "Here, $c$ is a constant and the $\\varphi$'s are the model parameters. The parameters can be found using least squares regression, maximum likelihood estimation, or the autocorrelation function:\n",
    "\n",
    "$$ \\rho(\\tau) = \\sum_{s=1}^p \\varphi_s \\rho(s - \\tau) $$\n",
    "  \n",
    "* **ARMA**.  This is just a combination of auto-regressive and moving average structures.  An $ARMA(p,q)$ process is given by the equations\n",
    "$$ X_t = \\sum_{r=1}^p \\varphi_r X_{t-r} + \\varepsilon_t + \\sum_{s=1}^q \\theta_s \\varepsilon_{t-s} $$\n",
    "Because we like mathematical shorthand, we will define a \"lag operator\" as\n",
    "$$ L(X)_t = X_{t-1} $$\n",
    "so that $L^2(X)_t = X_{t-2}$, etc ...\n",
    "Then we can rewrite the ARMA equation in terms of the lag operator as\n",
    "$$ \\varphi(L) X = \\theta(L) \\epsilon $$\n",
    "where the polynomials are given by \n",
    "$$\\varphi(z) = 1-\\sum_{r=1}^p \\varphi_r z^r$$\n",
    "and\n",
    "$$\\theta(z) = 1+\\sum_{s=1}^q\\theta_s z^s \\, .$$  It turns out this fancy notation will be useful later on. You can use a 2-step regression to estimate the model parameters. Alternatively you can use maximum likelihood estimation or Yule-Walker estimation. \n",
    "\n",
    "* **ARIMA**.  While ARMA is useful for handling stationary processes, it does not handle non-stationary proesses well.  Sometimes we can remove drift and seasonality directly, especially when there is a clear physical interpretation.  Othertimes, it is easier to just do so automatically by \"differentiating\".  We'll define the operator $\\nabla= (1-L)$ so that\n",
    "$$ (\\nabla X)_t = X_t - X_{t-1} $$\n",
    "Notice that\n",
    "$$ (\\nabla^2 X)_t = (1-L)^2  X_t = (1-2L+L^2)X_t = X_t - 2X_{t-1} + X_{t-2}. $$\n",
    "$ARIMA(p, d, q)$ is just $ARMA(p,q)$ applied to $(\\nabla^d X)_t$.  In our operator notation\n",
    "$$ (\\varphi \\circ \\nabla^d) (L) X = \\theta (L) \\varepsilon\\,. $$\n",
    "Notice that we can just rewrite $\\varphi' = \\varphi \\circ \\nabla^d$ as\n",
    "$$ \\varphi'(z) = 1- \\sum_{r=1}^{p+d} \\varphi'_r z^r = \\left(1-\\sum_{r=1}^p \\varphi_r z^r\\right) (1-z)^d $$\n",
    "and so we have that $ARIMA(p, d, q)$ is (basically) just an $ARMA(p+d,q)$.\n",
    "\n",
    "**Exercise**: Write a *residual regressor* that takes your best model and tries to fit and AR or ARMA model on the residuals using Statsmodels.  *Notes:*\n",
    "1.  We should always first remove seasonality and \"last data point\" features (the low-hanging fruit) before trying to perform analysis on the residuals.\n",
    "1.  It is not always natural to fit statsmodels into the framework\n",
    "1.  While the previous models could be tested by training models on the entire training set and then validating, for these autocorrelation models, we have to use sliding-window validation method mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Stochastic auto-regressive models\n",
    "\n",
    "Our time series has, very cleary, time-varying volatility.  To accurately model these effects, one often uses stochastic models.  To start you Googling, the basic auto-regressive examples are **ARCH/GARCH**.  \n",
    "\n",
    "Let us say just a little about these, leaving an example as an exercise to the reader.  In this type of model, the next time tick's value is drawn from a _distribution_ whose mean **and** standard deviation are modelled over time (and can, in general, be auto-regressive):\n",
    "\n",
    "$$ t_{i+1} = M(\\text{..factors..}) + \\sigma(\\text{..factors..}) \\epsilon_t $$\n",
    "\n",
    "where \n",
    "  - $M$ is some model for the mean (e.g., a linear model depending on some number of time lags of $t_{i}$ and moving averages in GARCH models);\n",
    "  - $\\sigma$ is some model for the standard deviation (as above in GARCH);\n",
    "  - and, $\\epsilon_t$ is a draw from a distribution having (conditional on the factors..) mean equal to zero, and standard deviation equal to one.  (In ARCH, this is a normal distribution.)\n",
    "  \n",
    "Stochastic models allow us to generate a range of future paths, for instance for modelling \"value at risk.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exit Tickets\n",
    "1. Describe how you would cross-validate a time series model.\n",
    "1. Describe the difference between auto-regressive and moving average terms in an ARMA model.\n",
    "1. Explain an FFT to a layperson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
