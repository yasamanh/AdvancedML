{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Natural Language Processing\n",
    "<!-- requirement: small_data/apple_fruit.txt -->\n",
    "<!-- requirement: small_data/apple_inc.txt -->\n",
    "<!-- requirement: small_data/ford_car.txt -->\n",
    "<!-- requirement: small_data/ford_crossing.txt -->\n",
    "<!-- requirement: small_data/window_glass.txt -->\n",
    "<!-- requirement: small_data/windows_ms.txt -->\n",
    "\n",
    "Natural language processing (NLP) is concerned with developing algorithms that enable computers to process or understand human (or \"natural\") language. In this notebook we will focus on using natural language processing to classify texts into different genres. Natural language processing is also concerned with language generation and translation, but these topics are rather advanced and are beyond the scope of this notebook. \n",
    "\n",
    "**EXAMPLE:**\n",
    "In a given source block of (prose) text, you may want to be able to tell apart Apple (the company) vs. apple (the fruit).  Ideally you would also be able to tell apart Ford vs ford, Windows vs windows, etc. via similar examples.\n",
    "\n",
    "**The type of learner**: We're going to choose to look at this as a _supervised classification_ problem.  There are also unsupervised approaches, but you have to make choices sometimes.  This means we need some \"marked up\" data:\n",
    "\n",
    "**The training dataset**: Having limited resources at our disposal, we're going to try to use Wikipedia's articles on the given topics as our chosen \"corpus\" of text.\n",
    "\n",
    "**The test dataset**: A good idea would be to mark up a small corpus by hand, or to use sentences culled from Wikipedia with the disambiguation coming from looking at the target of outgoing links.  We're not going to be that scientific for lack fo time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Text as a \"bag of words\"\n",
    "\n",
    "You can imagine that documents belonging to the same class will share more words in common with each other than with documents belonging to different classes. This idea is key to how we will construct our classifier. As such, as a first step, we will keep track of the word counts or frequencies for each document. \n",
    "\n",
    "  - Split the text into words\n",
    "  - Count how many times each word (in some fixed vocabulary) occurs\n",
    "  - _(Optionally)_ normalize the counts against some baseline\n",
    "  - _(Variant)_ Just do a binary \"yes / no\" for whether each word (in some vocabulary) is contained in the material\n",
    "  \n",
    "Let us count the words we see in documents (in this case, sentences) pertaining to apples (the fruit) and Apple (the company). Our first step will be to pull in the training (and test) data. We will want to clean both data on the way in: our goal is to have each text as a list of strings, one string for each sentence. We'll be using spaCy for this. It should already be installed, and its data set downloaded, on your box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When spaCy loads a document, it automatically tokenizes it into sentences and words. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a text document.\n",
      "spaCy requires it to be a unicode string.\n",
      "Word 3: text\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Here is a text document. spaCy requires it to be a unicode string.')\n",
    "# doc.sents is a generator producing sentences\n",
    "for sentence in doc.sents:\n",
    "    print sentence\n",
    "# doc can be indexed to find the individual words\n",
    "print \"Word 3:\", doc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's grab our documents from the apple and Apple wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Spit out (slightly cleaned up) sentences from a Wikipedia article.\n",
    "def wikipedia_to_sents(url):\n",
    "    soup = BeautifulSoup(urllib2.urlopen(url), 'lxml').find(attrs={'id':'mw-content-text'})\n",
    "    \n",
    "    # The text is littered by references like [n].  Drop them.\n",
    "    def drop_refs(s):\n",
    "        return ''.join(re.split('\\[\\d+\\]', s))\n",
    "    \n",
    "    paragraphs = [drop_refs(p.text) for p in soup.find_all('p')]\n",
    "    return [s.text for paragraph in paragraphs for s in nlp(paragraph).sents if len(s) > 2]\n",
    "\n",
    "fruit_sents = wikipedia_to_sents(\"http://en.wikipedia.org/wiki/Apple\")\n",
    "company_sents = wikipedia_to_sents(\"http://en.wikipedia.org/wiki/Apple_Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"The statement was released after the results from the company's probe into its suppliers' labor practices were published in early 2010.\",\n",
       " u\"Foxconn was not specifically named in the report, but Apple identified a series of serious labor violations of labor laws, including Apple's own rules, and some child labor existed in a number of factories.\",\n",
       " u'Apple committed to the implementation of changes following the suicides.',\n",
       " u'Also in 2010, workers in China planned to sue iPhone contractors over poisoning by a cleaner used to clean LCD screens.',\n",
       " u'One worker claimed that he and his coworkers had not been informed of possible occupational illnesses.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_sents[-105:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'In the wild, apples grow readily from seeds.',\n",
       " u'However, like most perennial fruits, apples are ordinarily propagated asexually by grafting.',\n",
       " u'This is because seedling apples are an example of \"extreme heterozygotes\", in that rather than inheriting DNA from their parents to create a new apple with those characteristics, they are instead significantly different from their parents.',\n",
       " u'Triploid cultivars have an additional reproductive barrier in that 3 sets of chromosomes cannot be divided evenly during meiosis, yielding unequal segregation of the chromosomes (aneuploids).',\n",
       " u'Even in the case when a triploid plant can produce a seed (apples are an example), it occurs infrequently, and seedlings rarely survive.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_sents[-105:-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Count Vectorizer\n",
    "\n",
    "Learning algorithms, however, prefer vectors of numbers, not text. When we do this, the output will be typically be a very large, but usually sparse, vector: The number of coordinates is the number of words in our dictionary, and the $i$-th coordinate entry is the number of occurrences of the $i$-th word.\n",
    "\n",
    "There's a reasonable implementation of this in the `CountVectorizer` class in sklearn.feature_extraction.text. See http://scikit-learn.org/stable/modules/classes.html#text-feature-extraction-ref for more detail on the options. When we use sklearn's `CountVectorizer`, we generate a matrix where each column corresponds to a word, each row corresponds to a document, and the values are the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(802, 3997)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words_vectorizer = CountVectorizer()\n",
    "\n",
    "counts = bag_of_words_vectorizer.fit_transform( fruit_sents + company_sents  )\n",
    "print counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "\n",
      "  (0, 1563)\t1\n",
      "  (0, 2738)\t1\n",
      "  (0, 3548)\t1\n",
      "  (0, 1998)\t1\n",
      "  (0, 1522)\t1\n",
      "  (0, 2063)\t1\n",
      "  (0, 516)\t1\n",
      "  (0, 1432)\t1\n",
      "  (0, 3131)\t1\n",
      "  (0, 1843)\t1\n",
      "  (0, 1040)\t1\n",
      "  (0, 1983)\t1\n",
      "  (0, 1175)\t1\n",
      "  (0, 626)\t1\n",
      "  (0, 1327)\t1\n",
      "  (0, 339)\t1\n",
      "  (0, 818)\t1\n",
      "  (0, 2892)\t1\n",
      "  (0, 2228)\t2\n",
      "  (0, 3718)\t2\n",
      "  (0, 367)\t2\n",
      "  (0, 3623)\t3\n",
      "  (1, 1620)\t1\n",
      "  (1, 3395)\t1\n",
      "  (1, 1694)\t1\n",
      "  :\t:\n",
      "  (800, 634)\t1\n",
      "  (800, 2817)\t1\n",
      "  (800, 380)\t1\n",
      "  (800, 3532)\t1\n",
      "  (800, 873)\t1\n",
      "  (800, 3948)\t1\n",
      "  (800, 3684)\t1\n",
      "  (800, 2319)\t1\n",
      "  (800, 3621)\t1\n",
      "  (800, 2351)\t1\n",
      "  (800, 3670)\t1\n",
      "  (800, 1522)\t1\n",
      "  (800, 1843)\t1\n",
      "  (800, 339)\t1\n",
      "  (800, 367)\t1\n",
      "  (801, 3968)\t1\n",
      "  (801, 1551)\t1\n",
      "  (801, 3168)\t1\n",
      "  (801, 3967)\t1\n",
      "  (801, 86)\t1\n",
      "  (801, 3081)\t1\n",
      "  (801, 418)\t1\n",
      "  (801, 346)\t1\n",
      "  (801, 1843)\t1\n",
      "  (801, 3623)\t1\n"
     ]
    }
   ],
   "source": [
    "# Note that counts is a **sparse** matrix.\n",
    "print counts.toarray()       #This is what it actually looks like.. there are non-zero entries, really!\n",
    "print \"\\n\", counts           # .. this is just describing the non-zero entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hashing Vectorizer\n",
    "\n",
    "When doing \"bag of words\" type techniques on a *large* corpus and without an existing vocabulary, there is a simple trick that is often useful.  The issue (and solution) is as follows: \n",
    "\n",
    " - The output is a feature vector, so that whenever we encounter a word we must look up which coordinate slot it is in.  A naive way would be to keep a list of all the words encountered so far, and look up each word when it is encountered.  Whenever we encounter a new word, we see if we've already seen it before and if not -- assign it a new number.  This requires storing all the words that we have seen in memory, cannot be done in parallel (because we'd have to share the hash-table of seen words), etc.\n",
    " - A **hash function** takes as input something complicated (like a string) and spits out a number, with the desired property being that different inputs *usually* produce different outputs.  (This is how hash tables are implemented, as the name suggests.)\n",
    " - So -- rather than exactly looking up the coordinate of a given word, we can just use its hash value (modulo a big size that we choose).  This is fast and parallelizes easily.  (There are some downsides: You cannot tell, after the fact, what word each of your feature actually corresponds to!)\n",
    " \n",
    "Scikit-learn includes `sklearn.feature_extraction.text.HashingVectorizer` to do this.  It behaves as almost a drop-in replacement for `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word importance: Term frequency–inverse document frequency (TF-IDF)\n",
    "\n",
    "Just using counts (or frequencies), as above, is clearly not great.  Both apples the fruit and Apple the company are enjoyed around the world!  We would like to find words that are common in one document, but not common in all of them.  This is the goal of the __tf-idf weighting__.  A precise definition is:\n",
    "\n",
    "\n",
    "  1. If $d$ denotes a document and $t$ denotes a term, then the _raw term frequency_ $\\mathrm{tf}^{raw}(t,d)$ is\n",
    "  $$ \\mathrm{tf}^{raw}(t,d) = \\text{the number of times the term $t$ occurs in the document $d$} $$\n",
    "  The vector of all term frequencies can optionally be _normalized_ either by dividing by the maximum of any single word's occurrence count ($L^1$) or by the Euclidean length of the vector of word occurrence counts ($L^2$).  Scikit-learn by default does this second one:\n",
    "  $$ \\mathrm{tf}(t,d) = \\mathrm{tf}^{L^2}(t,d) = \\frac{\\mathrm{tf}^{raw}(t,d)}{\\sqrt{\\sum_t \\mathrm{tf}^{raw}(t,d)^2}} $$\n",
    "  2. If $$ D = \\left\\{ d : d \\in D \\right\\} $$ is the set of possible documents, then  the _inverse document frequency_ is\n",
    "  $$ \\mathrm{idf}^{naive}(t,D) = \\log \\frac{\\# D}{\\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "  = \\log \\frac{\\text{count of all documents}}{\\text{count of those documents containing the term $t$}} $$\n",
    "  with a common variant being\n",
    "  $$ \\mathrm{idf}(t, D) = \\log \\frac{\\# D}{1 + \\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "   = \\log \\frac{\\text{count of all documents}}{1 + \\text{count of those documents containing the term $t$}} $$\n",
    "  (This second one is the default in scikit-learn. Without this tweak we would omit the $1+$ in the denominator and have to worry about dividing by zero if $t$ is not found in any documents.)\n",
    "  3. Finally, the weight that we assign to the term $t$ appearing in document $d$ and depending on the corpus of all documents $D$ is\n",
    "  $$ \\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\mathrm{idf}(t,D) $$\n",
    "  \n",
    "### TF-IDF Vectorizer\n",
    "The `CountVectorizer` and `HashingVectorizer` can be used with tf-idf by combining them with the `TfidfTransformer` (the `TfidfVectorizer` is the `CountVectorizer` together with the `TfidfTransformer`). For our application (where the training and test data is small), we may as well just use `TfidfVectorizer` -- but it is good to know that `HashingVectorizer` is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TFIDF close to 0 for a word very common in every document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'fruits', u'gb', u'generally', u'generation', u'glass']\n",
      "  (0, 273)\t0.549576096914\n",
      "  (0, 160)\t0.616903034937\n",
      "  (0, 140)\t0.28918122199\n",
      "  (0, 99)\t0.243654334759\n",
      "  (0, 43)\t0.281533668607\n",
      "  (0, 38)\t0.308451517468\n",
      "  (1, 296)\t0.445304622192\n",
      "  (1, 273)\t0.429002680864\n",
      "  (1, 160)\t0.481558527212\n",
      "  (1, 109)\t0.490979140851\n",
      "  (1, 99)\t0.380396321466\n",
      "  (2, 291)\t0.632920240853\n",
      "  (2, 273)\t0.514998978583\n",
      "  (2, 160)\t0.578089976368\n",
      "  (3, 299)\t0.390177490275\n",
      "  (3, 178)\t0.4416207104\n",
      "  (3, 109)\t0.466036513083\n",
      "  (3, 87)\t0.457094483644\n",
      "  (3, 27)\t0.476032851518\n",
      "  (4, 121)\t1.0\n",
      "  (5, 274)\t0.547663532004\n",
      "  (5, 142)\t0.570796550966\n",
      "  (5, 109)\t0.611764622316\n",
      "  (6, 273)\t0.378163619965\n",
      "  (6, 248)\t0.424491323711\n",
      "  :\t:\n",
      "  (797, 149)\t0.473088877072\n",
      "  (797, 112)\t0.473088877072\n",
      "  (797, 34)\t0.404690231275\n",
      "  (797, 29)\t0.364463637141\n",
      "  (797, 19)\t0.353565156264\n",
      "  (798, 259)\t0.344533253602\n",
      "  (798, 223)\t0.39101887204\n",
      "  (798, 133)\t0.408086676997\n",
      "  (798, 33)\t0.467041477833\n",
      "  (798, 31)\t0.429461923816\n",
      "  (798, 30)\t0.399125364897\n",
      "  (799, 259)\t0.263828493994\n",
      "  (799, 34)\t0.312494925484\n",
      "  (799, 33)\t0.357639932985\n",
      "  (799, 32)\t0.350777752191\n",
      "  (799, 31)\t0.328863154438\n",
      "  (799, 30)\t0.611265489381\n",
      "  (799, 2)\t0.316217680325\n",
      "  (800, 292)\t0.449834396329\n",
      "  (800, 271)\t0.449834396329\n",
      "  (800, 263)\t0.476911712296\n",
      "  (800, 169)\t0.387118222241\n",
      "  (800, 33)\t0.466896918433\n",
      "  (801, 29)\t0.717756809814\n",
      "  (801, 19)\t0.696293876151\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.en import STOPWORDS\n",
    "STOPWORDS\n",
    "\n",
    "ng_tfidf=TfidfVectorizer(max_features=300, \n",
    "                         ngram_range=(1,2), \n",
    "                         stop_words=STOPWORDS.union({u'apple', u'apples'}))\n",
    "ng_tfidf=ng_tfidf.fit( fruit_sents + company_sents )\n",
    "print ng_tfidf.get_feature_names()[100:105]\n",
    "print ng_tfidf.transform( fruit_sents + company_sents )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercises:\n",
    "  1. Imagine that $D$ consists of just two documents $D = \\{ d_1, d_2 \\}$ and that the word \"cultivar\" occurs in $d_1$ but not in $d_2$.  What is \n",
    "  $$ \\mathrm{tfidf}(\\mathrm{\"cultivar\"}, d_i, D)$$\n",
    "for each $i=1,2$?  For simplicity, use $\\mathrm{tf}^{raw}$ and the version of `idf` without the $1+$.  \n",
    "\n",
    "  2. Same question as 1, but now use $\\mathrm{tf}^{L^2}$ and the version of of `idf` with the $1+$.  \n",
    "\n",
    "  3. What happens to the tf-idf weighting of a word if it occurs in all (or all but one) documents?  Consider both forms of `idf`.\n",
    "  \n",
    "  4. In the example below, we consider each sentence as a separate document for the purpose of tf-idf.  What happens if you instead treat the input as just two documents, one for each starting article.\n",
    "  \n",
    "### Hints/Answers:\n",
    "  1. For $i=2$ it is zero.  For $i=1$, it is the number of occurrences of \"cultivar\" in $d_1$ multiplied by $\\log 2$.\n",
    "  2. For $i=2$, it is zero.  For $i=1$, it is .. also zero.\n",
    "  3. Answer: If the word occurs in all documents, and there are $N$ of them, then the $1+$ form weights `idf` by $\\log N/(N+1) < 0$ while the other form weights `idf` by $\\log N/N = 0$.  If the word occurs in all-but-one document, then the $1+$ form weights `idf` by $0$ while the other form weights it by $\\log N/(N-1) \\approx 1+1/N$.\n",
    "  4. It works less well, because of what's discussed in 3.  tf-idf doesn't work so well with few documents and where relevant words  occur (even if with wildly different frequencies!) in both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Document similarity metrics\n",
    "\n",
    "A common problem is looking up a document similar to a given snippet, or relatedly comparing two documents for similarity.  The above provides a simple method for this called __cosine similarity__:\n",
    "  - To each of the two documents $d_1, d_2$ in a corpus of documents $D$, assign its tf or tf-idf vector $$ (v_i)_{j} = \\mathrm{tfidf}( t_{j}, d_i, D ) $$\n",
    "  where $i$ ranges over indices for documents, and $j$ ranges over indices for terms in the vocabulary.\n",
    "  - To compare two documents, simply find the cosine of the angle between the vectors:\n",
    "  $$ \\frac{v_i \\cdot v_{i'}}{|v_i| |v_{i'}|} $$\n",
    "  \n",
    "(There's also a variant using binary vectors and Jaccard distance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Engineering your features\n",
    "\n",
    "There are some considerations to make when deciding which \"words\" or features to include in your classifier. In our example application, we might try to use:\n",
    "   - Capitalized of the word apple? (_a_pple vs _A_pple)    \n",
    "   - Pluralization of the word apple? (apples)\n",
    "   - Possessive form of the word apple? (Apple's)\n",
    "   - Presence (or frequency) of certain well-chosen words : Does (e.g.,) the word \"computer\" or \"fruit\" occur in the sentence?  (This feature regards the sentence as a simple __bag of words__ without regard to trying to parse its structure.)\n",
    "   - In addition to single words, we can also look for __n-grams__: Strings of n consecutive words.\n",
    "   - There are common techniques for determining which words / n-grams to look for.  One of them is called __tf-idf__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Stop words\n",
    "It's common to want to __omit__ certain common words when doing these counts -- \"a\", \"an\", and \"the\" are common enough so that their counts do not tend to give us any hints as to the meaning of documents.  Such words that we want to omit are called __stop words__ (they don't stop anything, though). NOTE: If you are using tfidf and not the word frequency to compute your document similarity, however, you don't need to exclude stop words.\n",
    "\n",
    "spaCy contains a standard list of such stop words for English in `spacy.en.STOPWORDS`.  In our application, we'd also want to include \"apple\" -- it is certainly not going to help us distinguish our two meanings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'a',\n",
       " u'about',\n",
       " u'above',\n",
       " u'across',\n",
       " u'after',\n",
       " u'afterwards',\n",
       " u'again',\n",
       " u'against',\n",
       " u'all',\n",
       " u'almost',\n",
       " u'alone',\n",
       " u'along',\n",
       " u'already',\n",
       " u'also',\n",
       " u'although',\n",
       " u'always',\n",
       " u'am',\n",
       " u'among',\n",
       " u'amongst',\n",
       " u'amoungst',\n",
       " u'amount',\n",
       " u'an',\n",
       " u'and',\n",
       " u'another',\n",
       " u'any',\n",
       " u'anyhow',\n",
       " u'anyone',\n",
       " u'anything',\n",
       " u'anyway',\n",
       " u'anywhere',\n",
       " u'are',\n",
       " u'around',\n",
       " u'as',\n",
       " u'at',\n",
       " u'back',\n",
       " u'be',\n",
       " u'became',\n",
       " u'because',\n",
       " u'become',\n",
       " u'becomes',\n",
       " u'becoming',\n",
       " u'been',\n",
       " u'before',\n",
       " u'beforehand',\n",
       " u'behind',\n",
       " u'being',\n",
       " u'below',\n",
       " u'beside',\n",
       " u'besides',\n",
       " u'between',\n",
       " u'beyond',\n",
       " u'bill',\n",
       " u'both',\n",
       " u'bottom',\n",
       " u'but',\n",
       " u'by',\n",
       " u'call',\n",
       " u'can',\n",
       " u'cannot',\n",
       " u'cant',\n",
       " u'co',\n",
       " u'computer',\n",
       " u'con',\n",
       " u'could',\n",
       " u'couldnt',\n",
       " u'cry',\n",
       " u'de',\n",
       " u'describe',\n",
       " u'detail',\n",
       " u'did',\n",
       " u'didn',\n",
       " u'do',\n",
       " u'does',\n",
       " u'doesn',\n",
       " u'doing',\n",
       " u'don',\n",
       " u'done',\n",
       " u'down',\n",
       " u'due',\n",
       " u'during',\n",
       " u'each',\n",
       " u'eg',\n",
       " u'eight',\n",
       " u'either',\n",
       " u'eleven',\n",
       " u'else',\n",
       " u'elsewhere',\n",
       " u'empty',\n",
       " u'enough',\n",
       " u'etc',\n",
       " u'even',\n",
       " u'ever',\n",
       " u'every',\n",
       " u'everyone',\n",
       " u'everything',\n",
       " u'everywhere',\n",
       " u'except',\n",
       " u'few',\n",
       " u'fifteen',\n",
       " u'fify',\n",
       " u'fill',\n",
       " u'find',\n",
       " u'fire',\n",
       " u'first',\n",
       " u'five',\n",
       " u'for',\n",
       " u'former',\n",
       " u'formerly',\n",
       " u'forty',\n",
       " u'found',\n",
       " u'four',\n",
       " u'from',\n",
       " u'front',\n",
       " u'full',\n",
       " u'further',\n",
       " u'get',\n",
       " u'give',\n",
       " u'go',\n",
       " u'had',\n",
       " u'has',\n",
       " u'hasnt',\n",
       " u'have',\n",
       " u'he',\n",
       " u'hence',\n",
       " u'her',\n",
       " u'here',\n",
       " u'hereafter',\n",
       " u'hereby',\n",
       " u'herein',\n",
       " u'hereupon',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'him',\n",
       " u'himself',\n",
       " u'his',\n",
       " u'how',\n",
       " u'however',\n",
       " u'hundred',\n",
       " u'i',\n",
       " u'ie',\n",
       " u'if',\n",
       " u'in',\n",
       " u'inc',\n",
       " u'indeed',\n",
       " u'interest',\n",
       " u'into',\n",
       " u'is',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'just',\n",
       " u'keep',\n",
       " u'kg',\n",
       " u'km',\n",
       " u'last',\n",
       " u'latter',\n",
       " u'latterly',\n",
       " u'least',\n",
       " u'less',\n",
       " u'ltd',\n",
       " u'made',\n",
       " u'make',\n",
       " u'many',\n",
       " u'may',\n",
       " u'me',\n",
       " u'meanwhile',\n",
       " u'might',\n",
       " u'mill',\n",
       " u'mine',\n",
       " u'more',\n",
       " u'moreover',\n",
       " u'most',\n",
       " u'mostly',\n",
       " u'move',\n",
       " u'much',\n",
       " u'must',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'name',\n",
       " u'namely',\n",
       " u'neither',\n",
       " u'never',\n",
       " u'nevertheless',\n",
       " u'next',\n",
       " u'nine',\n",
       " u'no',\n",
       " u'nobody',\n",
       " u'none',\n",
       " u'noone',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'nothing',\n",
       " u'now',\n",
       " u'nowhere',\n",
       " u'of',\n",
       " u'off',\n",
       " u'often',\n",
       " u'on',\n",
       " u'once',\n",
       " u'one',\n",
       " u'only',\n",
       " u'onto',\n",
       " u'or',\n",
       " u'other',\n",
       " u'others',\n",
       " u'otherwise',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'out',\n",
       " u'over',\n",
       " u'own',\n",
       " u'part',\n",
       " u'per',\n",
       " u'perhaps',\n",
       " u'please',\n",
       " u'put',\n",
       " u'quite',\n",
       " u'rather',\n",
       " u're',\n",
       " u'really',\n",
       " u'regarding',\n",
       " u'same',\n",
       " u'say',\n",
       " u'see',\n",
       " u'seem',\n",
       " u'seemed',\n",
       " u'seeming',\n",
       " u'seems',\n",
       " u'serious',\n",
       " u'several',\n",
       " u'she',\n",
       " u'should',\n",
       " u'show',\n",
       " u'side',\n",
       " u'since',\n",
       " u'sincere',\n",
       " u'six',\n",
       " u'sixty',\n",
       " u'so',\n",
       " u'some',\n",
       " u'somehow',\n",
       " u'someone',\n",
       " u'something',\n",
       " u'sometime',\n",
       " u'sometimes',\n",
       " u'somewhere',\n",
       " u'still',\n",
       " u'such',\n",
       " u'system',\n",
       " u'take',\n",
       " u'ten',\n",
       " u'than',\n",
       " u'that',\n",
       " u'the',\n",
       " u'their',\n",
       " u'them',\n",
       " u'themselves',\n",
       " u'then',\n",
       " u'thence',\n",
       " u'there',\n",
       " u'thereafter',\n",
       " u'thereby',\n",
       " u'therefore',\n",
       " u'therein',\n",
       " u'thereupon',\n",
       " u'these',\n",
       " u'they',\n",
       " u'thick',\n",
       " u'thin',\n",
       " u'third',\n",
       " u'this',\n",
       " u'those',\n",
       " u'though',\n",
       " u'three',\n",
       " u'through',\n",
       " u'throughout',\n",
       " u'thru',\n",
       " u'thus',\n",
       " u'to',\n",
       " u'together',\n",
       " u'too',\n",
       " u'top',\n",
       " u'toward',\n",
       " u'towards',\n",
       " u'twelve',\n",
       " u'twenty',\n",
       " u'two',\n",
       " u'un',\n",
       " u'under',\n",
       " u'unless',\n",
       " u'until',\n",
       " u'up',\n",
       " u'upon',\n",
       " u'us',\n",
       " u'used',\n",
       " u'using',\n",
       " u'various',\n",
       " u'very',\n",
       " u'via',\n",
       " u'was',\n",
       " u'we',\n",
       " u'well',\n",
       " u'were',\n",
       " u'what',\n",
       " u'whatever',\n",
       " u'when',\n",
       " u'whence',\n",
       " u'whenever',\n",
       " u'where',\n",
       " u'whereafter',\n",
       " u'whereas',\n",
       " u'whereby',\n",
       " u'wherein',\n",
       " u'whereupon',\n",
       " u'wherever',\n",
       " u'whether',\n",
       " u'which',\n",
       " u'while',\n",
       " u'whither',\n",
       " u'who',\n",
       " u'whoever',\n",
       " u'whole',\n",
       " u'whom',\n",
       " u'whose',\n",
       " u'why',\n",
       " u'will',\n",
       " u'with',\n",
       " u'within',\n",
       " u'without',\n",
       " u'would',\n",
       " u'yet',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.en import STOPWORDS\n",
    "STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True False True\n"
     ]
    }
   ],
   "source": [
    "# o_O\n",
    "print 'fify' in STOPWORDS\n",
    "print 'six' in STOPWORDS, 'seven' in STOPWORDS, 'eight' in STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'000', u'10', u'100', u'12', u'16', u'1984', u'1997', u'20', u'2001', u'2006', u'2007', u'2008', u'2009', u'2010', u'2011', u'2012', u'2013', u'2014', u'2015', u'2016', u'2017', u'30', u'500', u'800', u'according', u'added', u'america', u'american', u'announced', u'annual', u'app', u'apples', u'application', u'applications', u'apps', u'april', u'august', u'away', u'began', u'best', u'billion', u'board', u'brand', u'business', u'california', u'called', u'camera', u'campus', u'center', u'century', u'ceo', u'characteristics', u'china', u'climate', u'commercial', u'companies', u'company', u'computers', u'conditions', u'consumer', u'consumers', u'content', u'continues', u'cook', u'corporate', u'corporation', u'country', u'cultivar', u'cultivars', u'cultivated', u'current', u'cut', u'data', u'davidson', u'day', u'days', u'december', u'design', u'designed', u'desktop', u'developed', u'development', u'device', u'devices', u'different', u'digital', u'directly', u'disease', u'display', u'early', u'efforts', u'electronics', u'employees', u'end', u'energy', u'europe', u'featured', u'features', u'february', u'following', u'foxconn', u'free', u'fruit', u'gb', u'generally', u'generation', u'glass', u'global', u'goddess', u'golden', u'government', u'graphics', u'greek', u'green', u'grown', u'hardware', u'health', u'help', u'high', u'higher', u'ii', u'imac', u'important', u'improved', u'inch', u'include', u'included', u'includes', u'including', u'increase', u'individuals', u'industry', u'information', u'internal', u'internet', u'introduced', u'ios', u'ipad', u'iphone', u'iphones', u'ipod', u'itunes', u'january', u'jobs', u'july', u'june', u'known', u'labor', u'large', u'largest', u'late', u'later', u'launch', u'launched', u'led', u'life', u'line', u'lisa', u'located', u'logo', u'long', u'low', u'lower', u'mac', u'macbook', u'macintosh', u'macworld', u'major', u'malus', u'management', u'manufacturer', u'manufacturing', u'march', u'market', u'marketing', u'media', u'microsoft', u'million', u'mini', u'models', u'modern', u'months', u'music', u'needed', u'new', u'north', u'november', u'number', u'numerous', u'october', u'online', u'opened', u'operating', u'operations', u'organic', u'original', u'os', u'particularly', u'pay', u'people', u'period', u'personal', u'pests', u'plant', u'platform', u'player', u'points', u'policy', u'portable', u'power', u'practices', u'president', u'price', u'pro', u'problems', u'processor', u'produce', u'produced', u'producing', u'product', u'production', u'products', u'profit', u'profits', u'program', u'public', u'quarter', u'range', u'rate', u'record', u'release', u'released', u'renewable', u'replaced', u'report', u'reported', u'research', u'retail', u'revenue', u'rootstocks', u'run', u'sales', u'samsung', u'saw', u'sculley', u'second', u'seeds', u'sell', u'september', u'series', u'service', u'share', u'significant', u'size', u'software', u'sold', u'specifically', u'square', u'stated', u'states', u'steve', u'stock', u'storage', u'store', u'stores', u'success', u'suppliers', u'support', u'tax', u'team', u'technology', u'time', u'times', u'titles', u'took', u'total', u'touch', u'tree', u'trees', u'tv', u'uk', u'united', u'unveiled', u'update', u'use', u'user', u'users', u'version', u'vice', u'video', u'watch', u'website', u'work', u'workers', u'working', u'world', u'worldwide', u'wozniak', u'year', u'years']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<206x300 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 592 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vocabulary *can* be built for you.  \n",
    "#\n",
    "# For instance, here we'll compute and then use the top 300 words by frequency -- *ignoring*\n",
    "# the so-called \"stopwords\": these are words like \"a\", \"and\", \"the\" that are very common\n",
    "# \"apple\" is not useful for distinguishing the two, but is common, so add it as a stopword.\n",
    "#\n",
    "# Nevertheless, this method is probably NOT GOOD.  See tf-idf instead.\n",
    "counter=CountVectorizer(max_features=300,\n",
    "                        stop_words=STOPWORDS.union({u'apple'}))\n",
    "counter=counter.fit( fruit_sents + company_sents )\n",
    "print counter.get_feature_names()\n",
    "\n",
    "# Now we can use it with that vectorizer, like so...\n",
    "counter.transform(company_sents)\n",
    "counter.transform(fruit_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Stemming\n",
    "\n",
    "In our original hand-built vocabulary, we had to include both \"apple\" and \"apples\".  It would have been useful to identify them as one word.\n",
    "\n",
    "This is not limited to just trailing \"s\" characters: e.g., the words \"carry\", \"carries\", \"carrying\", and \"carried\" all carry -- roughly -- the same meaning.  The process of replacing them by a common root, or **stem**, is called stemming -- the stem will not, in general, be a full word itself.\n",
    "\n",
    "There's a related process called **lemmatization**: The analog of the \"stem\" here _is_ an actual word.  After spaCy processes some text, the `lemma_` property of each word contains the lemmatized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'carry', u'carry', u'carry', u'carry']\n",
      "[u'eat', u'eat', u'eat', u'eat']\n",
      "the quick brown fox jump over the lazy dog .   i can not believe it ' not butter .   i try to ford the river and my unfortunate ox die .\n"
     ]
    }
   ],
   "source": [
    "print [w.lemma_ for w in nlp(u'carry carries carrying carried')]\n",
    "print [w.lemma_ for w in nlp(u'eat eating eaten ate')]\n",
    "print ' '.join(w.lemma_ for w in nlp(u\"The quick brown fox jumped over the lazy dog.  \"\n",
    "                                     u\"I can't believe it's not butter.  \"\n",
    "                                     u\"I tried to ford the river and my unfortunate oxen died.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can tell our bag-of-words counters (or tf-idf) to run on lemmatized text.  This way it won't have to include both e.g., 'apple' and 'apples':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'  ', u'   billion', u'   gb', u'   million', u'\"', u'\" ,', u'\" .', u'$', u'%', u\"'\", u\"' ,\", u\"'s\", u'(', u')', u') ,', u') .', u',', u', \"', u\", '\", u\", 's\", u', -', u', .', u', 2011', u', 2012', u', 2016', u', announce', u', feature', u', include', u', introduce', u', job', u', release', u'-', u'- -', u'.', u'. \"', u'10', u'100', u'100 %', u'1997', u'2', u'2001', u'2006', u'2007', u'2007 ,', u'2008', u'2008 ,', u'2009', u'2010', u'2010 ,', u'2011', u'2011 ,', u'2012', u'2012 ,', u'2013', u'2013 ,', u'2014', u'2014 ,', u'2015', u'2015 ,', u'2016', u'2016 ,', u'2017', u'3', u'30', u'3g', u'4', u'5', u'6', u'7', u'8', u'9', u':', u';', u']', u'accord', u'add', u'allow', u'american', u'announce', u'app', u'app store', u'application', u'april', u'august', u'begin', u'best', u'billion', u'board', u'brand', u'bring', u'build', u'camera', u'campus', u'cause', u'center', u'century', u'ceo', u'change', u'china', u'climate', u'commercial', u'company', u\"company 's\", u'company .', u'consumer', u'continue', u'control', u'cook', u'corporation', u'country', u'create', u'cultivar', u'data', u'day', u'design', u'desktop', u'develop', u'development', u'device', u'different', u'digital', u'disease', u'display', u'early', u'eat', u'effort', u'electronics', u'employee', u'end', u'energy', u'europe', u'event', u'facility', u'feature', u'february', u'focus', u'follow', u'form', u'foxconn', u'free', u'fruit', u'gb', u'generally', u'generation', u'golden', u'green', u'group', u'grow', u'hardware', u'help', u'high', u'high -', u'ii', u'imac', u'include', u'increase', u'individual', u'introduce', u'ipad', u'iphone', u'ipod', u'itunes', u'itunes store', u'january', u'job', u'july', u'june', u'know', u'large', u'late', u'later', u'launch', u'lead', u'line', u'logo', u'long', u'low', u'mac', u'mac o', u'macintosh', u'major', u'malus', u'manufacturer', u'march', u'market', u'market .', u'medium', u'microsoft', u'million', u'model', u'month', u'music', u'need', u'new', u'north', u'november', u'number', u'o', u'october', u'offer', u'online', u'open', u'operation', u'organic', u'original', u'pay', u'people', u'period', u'personal', u'phone', u'place', u'plant', u'player', u'point', u'power', u'practice', u'president', u'price', u'pro', u'processor', u'produce', u'product', u'product ,', u'product .', u'production', u'profit', u'program', u'project', u'provide', u'public', u'purchase', u'quarter', u'rank', u'reaction', u'receive', u'record', u'release', u'renewable', u'renewable energy', u'replace', u'report', u'research', u'result', u'retail', u'revenue', u'rootstock', u'run', u'sale', u'samsung', u'saw', u'seed', u'sell', u'september', u'series', u'service', u'share', u'significant', u'size', u'software', u'source', u'standard', u'state', u'steve', u'steve job', u'stock', u'storage', u'store', u'store ,', u'success', u'suicide', u'supplier', u'support', u'tax', u'team', u'technology', u'time', u'total', u'touch', u'tree', u'tv', u'u.', u'u. .', u'united', u'united state', u'unveil', u'update', u'user', u'version', u'vice', u'video', u'watch', u'work', u'worker', u'world', u\"world 's\", u'worldwide', u'x', u'year', u'year ,', u'yield']\n"
     ]
    }
   ],
   "source": [
    "default_tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "    \n",
    "def tokenize_lemma(text):\n",
    "    return [w.lemma_ for w in nlp(text)]\n",
    "\n",
    "stop_words_lemma = set(w.lemma_ for w in nlp(' '.join(STOPWORDS)))\n",
    "\n",
    "ng_stem_tfidf = TfidfVectorizer(max_features=300, \n",
    "                                ngram_range=(1,2), \n",
    "                                stop_words=stop_words_lemma.union({\"apple\"}),\n",
    "                                tokenizer=tokenize_lemma)\n",
    "ng_stem_tfidf = ng_stem_tfidf.fit(fruit_sents + company_sents)\n",
    "\n",
    "ng_stem_vocab = ng_stem_tfidf.get_feature_names()\n",
    "print ng_stem_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization refers to splitting the text into pieces, in this case into sentences and into words. Instead of looking at just single words, it is also useful to look at **n-grams**: These are n-word long sequences of words (i.e., each of \"farmer's market\", \"market share\", and \"farm share\" is a 2-gram).\n",
    "\n",
    "The exact same sort of counting techniques apply.  The `CountVectorizer` function has built in support for this, too:\n",
    "\n",
    "If you pass it the `ngram_range=(m, M)` then it will count $n$-grams with  $m \\leq n \\leq M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'000 square', u'000 time', u'100 renewable', u'19th century', u'2007 jobs', u'2011 jobs', u'2016 update', u'21 2016', u'27 2010', u'30 2012', u'33182 122', u'37 33182', u'3g service', u'40 gb', u'48 total', u'4s iphone', u'500 known', u'700 billion', u'800 000', u'84 million', u'a4 processor', u'according report', u'added support', u'adverse reactions', u'aim alliance', u'alexander great', u'app store', u'april 24', u'august 24', u'backlit lcd', u'bear fruit', u'begin producing', u'best global', u'billion annual', u'billion cash', u'billion dollar', u'billion downloads', u'birch pollen', u'birch syndrome', u'board directors', u'brand loyalty', u'brands report', u'bud sports', u'carbon dioxide', u'carbon footprint', u'cash reserves', u'central asia', u'ceo began', u'ceo michael', u'ceo tim', u'citation needed', u'climate counts', u'companies time', u'company history', u'company product', u'company revenue', u'computers saw', u'computers use', u'consumer electronics', u'consumer market', u'control size', u'controlled atmosphere', u'cultivars bred', u'cultivars vary', u'cut pro', u'data centers', u'davidson notes', u'desktop publishing', u'devices running', u'didi chuxing', u'different cultivars', u'displays arsenic', u'doctor away', u'downloads june', u'dr dre', u'dwarf rootstocks', u'effective tax', u'electronics companies', u'electronics manufacturer', u'employees work', u'end era', u'featured company', u'february 2015', u'fifth avenue', u'final cut', u'financial success', u'fiscal year', u'forbidden fruit', u'form allergy', u'fortune 500', u'fourth generation', u'foxconn suicides', u'free glass', u'free led', u'free pvc', u'fruit collection', u'fruits nuts', u'fungal bacterial', u'garden hesperides', u'gb hard', u'gb tv', u'gb version', u'generation ipad', u'genus malus', u'gil amelio', u'golden apples', u'hardware products', u'high level', u'high price', u'high profile', u'high right', u'high yields', u'higher price', u'highly successful', u'hong kong', u'ibm motorola', u'ilife suite', u'imac g3', u'imac hello', u'inch display', u'inch model', u'inch screen', u'incorporated january', u'industry standards', u'information technology', u'intellectual property', u'internal memo', u'introduced iphone', u'introduced new', u'ios app', u'ipad iphone', u'ipad mini', u'ipad pro', u'iphone 3gs', u'iphone 4s', u'ipod touch', u'itunes library', u'itunes store', u'january 2007', u'jobs announced', u'jobs resigned', u'jobs steve', u'john sculley', u'jonathan ive', u'journal reported', u'june 2009', u'labor practices', u'largest mobile', u'later year', u'lcd displays', u'logo designed', u'long lines', u'low tax', u'lower cost', u'mac app', u'mac os', u'mac personal', u'macbook pro', u'macintosh ii', u'macintosh lc', u'macworld conference', u'macworld expo', u'major product', u'malus pumila', u'malus sieversii', u'manufacturer samsung', u'march 2011', u'march 2012', u'march 2016', u'march 21', u'market cap', u'market capitalization', u'market leader', u'market share', u'marketing company', u'marking end', u'media player', u'media streaming', u'medical leave', u'new campus', u'new cultivars', u'new ipad', u'new store', u'new york', u'norse mythology', u'north america', u'north american', u'north carolina', u'northern europe', u'notes connection', u'november 10', u'november 2011', u'november 2012', u'number fungal', u'numerous acquisitions', u'occurred january', u'october 2011', u'october 2013', u'october 2016', u'october 23', u'old cultivars', u'online music', u'online services', u'online store', u'operating systems', u'operations run', u'optional 3g', u'organic methods', u'os applications', u'os new', u'outside united', u'outstanding shares', u'paper product', u'personal computers', u'portable music', u'position ceo', u'power mac', u'power source', u'president retail', u'press release', u'previous year', u'price points', u'price tag', u'priced products', u'product line', u'profit margins', u'released iphone', u'renewable energy', u'renewable sources', u'resulting tree', u'retail stores', u'retina display', u'revenue world', u'right policy', u'ron johnson', u'ronald wayne', u'rootstocks today', u'run renewable', u'running ios', u'safari web', u'san francisco', u'senior vice', u'september 2016', u'silicon valley', u'size tree', u'small amounts', u'software includes', u'software titles', u'solar energy', u'spend time', u'split adjusted', u'square feet', u'square foot', u'square san', u'staff designers', u'stainless steel', u'steve jobs', u'steve wozniak', u'stock price', u'store concepts', u'store design', u'store digital', u'store openings', u'store store', u'store world', u'stores 2001', u'stores 22', u'street journal', u'technology company', u'themed version', u'thousands years', u'tim cook', u'time employees', u'tonnes china', u'trade shows', u'tree grown', u'trees large', u'tv digital', u'tv runs', u'uk national', u'ultimate size', u'union square', u'united kingdom', u'united states', u'university reading', u'use worldwide', u'user interface', u'valuable brand', u'valued 700', u'vice president', u'video editing', u'wall street', u'wi fi', u'wide range', u'wild ancestor', u'working conditions', u'world largest', u'year introduced', u'york city', u'york times']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<206x300 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 135 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_counter=CountVectorizer(max_features=300, \n",
    "                           ngram_range=(2,2), \n",
    "                           stop_words=STOPWORDS.union({u'apple', u'Apple'}))\n",
    "ng_counter=ng_counter.fit( fruit_sents + company_sents  )\n",
    "print ng_counter.get_feature_names()\n",
    "\n",
    "# Now we can use it with that vectorizer, like so...\n",
    "ng_counter.transform(company_sents)\n",
    "ng_counter.transform(fruit_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problems with this, two grams are n squared which is biggest problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# we have 300 \"features\"\n",
    "print len(ng_counter.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part of speech tagging\n",
    "Consider the \"Ford\" vs \"ford\" example.  As a human being, the easiest way to tell these apart is that Ford is a __noun__ while ford is a __verb__.\n",
    "\n",
    "Fortunately, spaCy also has a part-of-speech tagger: You give it a sentence, and it tries to tag the parts of speech (e.g., noun, verb, adjective, etc.).  The broad category is given in the `.pos_` property, while a more detailed description, using the [UPenn Treebank Tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), is in the `.tag_` property.\n",
    "\n",
    "(N.B. Nothing's perfect -- the tagger will make mistakes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s1 = u\"I tried to ford the river, and my unfortunate oxen died.\"\n",
    "s2 = u\"Henry Ford built factories to facilitate the construction of the Ford automobile.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[(w.text, w.pos_, w.tag_) for w in nlp(s1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[(w.text, w.pos_, w.tag_) for w in nlp(s2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Capitalization, punctuation, etc.\n",
    "There are the obvious features that we had in mind...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the Classifier\n",
    "Disclaimer: This version is actually pretty bad -- it uses many of the right ideas, but puts them together pretty poorly (and with fairly little available data). Let's first start by creating a function to retrieve text from a url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def wikipedia_to_sents(url):\n",
    "    \"\"\"\n",
    "    Retrieves a URL from wikipedia, and returns a list of sentences (of at least 3 words) in the body text.\n",
    "    \"\"\"\n",
    "    files_by_url = {\n",
    "      \"http://en.wikipedia.org/wiki/Ford_(crossing)\": \"ford_crossing.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Ford\": \"ford_car.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Apple\": \"apple_fruit.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Apple_Inc.\": \"apple_inc.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Window\": \"window_glass.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Microsoft_Windows\": \"windows_ms.txt\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(\"small_data/{}\".format(files_by_url[url])) as wiki_file:\n",
    "            soup = BeautifulSoup(wiki_file.read(), 'lxml').find(attrs={'id':'mw-content-text'})\n",
    "    except KeyError:\n",
    "        soup = BeautifulSoup(urllib2.urlopen(url), 'lxml').find(attrs={'id':'mw-content-text'})\n",
    "    \n",
    "    # The text is littered by references like [n].  Drop them.\n",
    "    def drop_refs(s):\n",
    "        return ''.join( re.split('\\[\\d+\\]', s) )\n",
    "    \n",
    "    paragraphs = [drop_refs(p.text) for p in soup.find_all('p')]\n",
    "    return [s.text for paragraph in paragraphs for s in nlp(paragraph).sents if len(s) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll then perform feature engineering in a transformer class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AdHocFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Given a keyword (e.g., \"apple\"), will transform documents into an\n",
    "    encoding of several ad hoc features of each occurrences of the keyword:\n",
    "        - If the keyword is capitalized\n",
    "        - If it is plural\n",
    "        - If it is possessive (in the stupid sense of being followed by 's)\n",
    "        - If the keyword is a verb (e.g., for Ford vs ford)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keyword):\n",
    "        self.keyword = nlp(keyword)[0].lemma_\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.asarray([self.transform_doc(x) for x in X])\n",
    "    \n",
    "    def feature_posessive(self, doc):\n",
    "        ## N.B. spaCy will tokenize \"Apple's\" as [\"Apple\", \"'s\"]\n",
    "        hits = [i for i, word in enumerate(doc) if word.lemma_ == self.keyword]\n",
    "        return sum((i + 1) < len(doc) and doc[i+1].text == \"'s\" for i in hits)\n",
    "    \n",
    "    def transform_doc(self, row):\n",
    "        doc = nlp(row)\n",
    "        words = [word for word in doc if word.lemma_ == self.keyword]\n",
    "        return [sum(word.is_title for word in words),\n",
    "                sum(word.tag_ in (u'NNS', u'NNPS') for word in words),\n",
    "                self.feature_posessive(doc),\n",
    "                sum(word.pos_ == u'VERB' for word in words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we'll make our classifier. To do this, we will use a multinomial **Naive Bayes** model. Detailed information on Naive Bayes can be found in the Naive Bayes notebook, but we'll briefly describe it here. The goal is, given a set of observed features $X_1, \\ldots, X_p$, to find the label $Y$ with the maximum conditional probability. In other words, we know what our distributions of words ($X$'s) should look like for a given genre ($Y$) from our training data, and we would like to use this information to find the genre of a body of text ($Y$) given its words ($X$'s) for new data. Bayes theorem gives us a way to compute the latter conditional probability from the former. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "def make_classifier(base_word, meaning1, meaning2):\n",
    "    \"\"\"\n",
    "    Given\n",
    "        - a base word (e.g., \"apple\", \"ford\") that can have ambiguous meaning\n",
    "        - a pair meaning1 = (name1, url1) of a label for the first meaning, and a Wikipedia URL for it\n",
    "        - a pair meaning2 = ... for the other meaning\n",
    "    Returns a classifier that predicts the meaning\n",
    "    \"\"\"\n",
    "    name1, url1 = meaning1\n",
    "    name2, url2 = meaning2\n",
    "    sents1 = wikipedia_to_sents(url1)\n",
    "    sents2 = wikipedia_to_sents(url2)\n",
    "    \n",
    "    stop_words_lemma = set(w.lemma_ for w in nlp(' '.join(STOPWORDS)))\n",
    "    def tokenize_lemma(text):\n",
    "        return [w.lemma_ for w in nlp(text)]\n",
    "\n",
    "    features = FeatureUnion([('stem_vectorizer',\n",
    "                              TfidfVectorizer(ngram_range=(1,2),\n",
    "                                              stop_words=stop_words_lemma.union({base_word}),\n",
    "                                              tokenizer=tokenize_lemma)),\n",
    "                             ('ad_hoc', AdHocFeatures(base_word))])\n",
    "    pipe = Pipeline([('features', features),\n",
    "                     ('classifier', MultinomialNB())])\n",
    "\n",
    "    # Build the training data\n",
    "    train_res  = [name1] * len(sents1) + [name2] * len(sents2)\n",
    "    \n",
    "    return pipe.fit(sents1 + sents2, train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fruit' 'company' 'company' 'company' 'company' 'fruit' 'company' 'fruit']\n"
     ]
    }
   ],
   "source": [
    "base_word = u\"apple\"\n",
    "options = [ (\"fruit\", \"http://en.wikipedia.org/wiki/Apple\"),\n",
    "            (\"company\", \"http://en.wikipedia.org/wiki/Apple_Inc.\") ]\n",
    "print make_classifier(base_word, *options).predict([\n",
    "    u\"I'm baking a pie with my granny smith apples.\",\n",
    "    u\"I looked up the recipe on my Apple iPhone.\",\n",
    "    u\"The apple pie recipe is on my desk.\",\n",
    "    u\"How is Apple's stock doing?\",\n",
    "    u\"I'm drinking apple juice.\",\n",
    "    u\"I have three apples.\",\n",
    "    u\"Steve Jobs is the CEO of apple.\",\n",
    "    u\"Steve Jobs likes to eat apples.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also do this for other classes of text documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['software' 'building' 'building' 'building' 'building' 'building']\n"
     ]
    }
   ],
   "source": [
    "base_word = u\"windows\"\n",
    "options = [ (\"building\", \"http://en.wikipedia.org/wiki/Window\"),\n",
    "            (\"software\", \"http://en.wikipedia.org/wiki/Microsoft_Windows\") ]\n",
    "print make_classifier(base_word, *options).predict([\n",
    "    u\"Bill Gates was involved with Windows.\",\n",
    "    u\"Could you open the window?\",\n",
    "    u\"The 'broken window' theory related broken windows to increases in crime rate.\",\n",
    "    u\"The windows are all made of shatter-proof glass.\",\n",
    "    u\"Could you install windows on your computer?\",\n",
    "    u\"Could you install windows on your house?\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company' 'company' 'company' 'crossing']\n"
     ]
    }
   ],
   "source": [
    "base_word = u\"ford\"\n",
    "options = [ (\"crossing\", \"http://en.wikipedia.org/wiki/Ford_(crossing)\"),\n",
    "            (\"company\", \"http://en.wikipedia.org/wiki/Ford\") ]\n",
    "print make_classifier(base_word, *options).predict([\n",
    "    u\"I tried to ford the river and my unfortunate oxen died.\",\n",
    "    u\"Ford makes cars, though their quality is sometimes in dispute.\",\n",
    "    u\"The Ford Mustang is an iconic automobile.\",\n",
    "    u\"The river crossing was shallow, but we could not ford it.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercises / Brainstorming for Improvement:\n",
    "Change the code to use just the ad hoc features. How does this change the results? Why do you think this is?\n",
    "Same question as 1, but for the tf-idf features.\n",
    "Change the formation of tf-idf features as follows -- when doing the tf-idf weighting (in the call to fit appearing in make_ng_stem_vectorizer) we pass in the sentences as separate documents. How do the results change if we pass them in as just two documents?\n",
    "What ideas do you think could improve the performance of this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exit Tickets\n",
    "1. What are some other options for modeling with text data besides bag of words?\n",
    "1. How would you account for the fact that word meanings change over time?\n",
    "1. How do stopwords, stemming, and limiting the # features affect variance-bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#answers: 2. lemmatizeing and map older meanings to the older versions....\n",
    "#3. reduce overfitting and noise. stopwords reduce bias, so better job fitting signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some ideas / hints for the exercises:\n",
    "  - A key problem with the model is the small amount of training data.  At the least, we could follow links from the given Wikipedia articles.  Better would be to find other sources that directly use the words Apple/apple.\n",
    "  - In this specific case (apple/Apple) we would do better by using a few human created absolute rules _first_: e.g., typos aside -- apple's will always refer to the company and apples to the fruit, so we do not need to run a more complicated learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Additional NLP topics and resources\n",
    "\n",
    "Natural language processing is a big field.  We only (really) talked about a few tools and techniques.  Here are some other terms that are relevant:\n",
    "\n",
    " - Context free grammars (and probabilistic context free grammars): This is a simple and basic technique for parsing.  \n",
    " - [Word2vec](https://code.google.com/p/word2vec/) is a popular tool for creating a vectorized representation of a text corpus. The learned vectors can then be used to identify/predict words similar to a target, or even (weakly) to reason by analogy. For example, vector('Paris') - vector('France') + vector('Italy') results in a vector that is very close to vector('Rome'), and vector('king') - vector('man') + vector('woman') is close to vector('queen').\n",
    " \n",
    " To use Word2vec in Python (and get the computation speed improvements) look at [gensim](https://radimrehurek.com/gensim/models/word2vec.html) and [cython](https://github.com/cython/cython/wiki/Installing). This [Git repo](https://github.com/danielfrg/word2vec) is an alternative way to access the algorithm. You might also use this [Kaggle competition](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors) as a reference.  spaCy gives words a `.vector` property based on [a particular](https://spacy.io/docs#token-distributional) word2vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
